


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pytorchvideo.models.resnet &mdash; PyTorchVideo  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/favicon.png"/>
  
  
  
    <link rel="canonical" href="https://pytorchvideo.org/docs/_modules/pytorchvideo/models/resnet.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorchvideo.org" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorchvideo.org">Home</a>
          </li>
          <li>
            <a href="https://pytorchvideo.org/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/models.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/model_zoo.html">Model Zoo and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/models/index.html">Models API</a></li>
</ul>
<p class="caption"><span class="caption-text">Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/data.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/data/index.html">Data API</a></li>
</ul>
<p class="caption"><span class="caption-text">Transforms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/transforms.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/transforms/index.html">Transforms API</a></li>
</ul>
<p class="caption"><span class="caption-text">Layers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/layers.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/layers/index.html">Layers API</a></li>
</ul>
<p class="caption"><span class="caption-text">Accelerator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/accelerator.html">Overview</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>pytorchvideo.models.resnet</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for pytorchvideo.models.resnet</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.layers.utils</span> <span class="kn">import</span> <span class="n">set_attributes</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.models.head</span> <span class="kn">import</span> <span class="n">create_res_basic_head</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.models.net</span> <span class="kn">import</span> <span class="n">Net</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.models.stem</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">create_acoustic_res_basic_stem</span><span class="p">,</span>
    <span class="n">create_res_basic_stem</span><span class="p">,</span>
<span class="p">)</span>


<div class="viewcode-block" id="create_bottleneck_block"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.create_bottleneck_block">[docs]</a><span class="k">def</span> <span class="nf">create_bottleneck_block</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Convolution configs.</span>
    <span class="n">dim_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_inner</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">conv_a_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">conv_a</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_b_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">conv_b_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">conv_b_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b_num_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">conv_b_dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_c</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="c1"># Norm configs.</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
    <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">norm_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># Activation configs.</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Bottleneck block: a sequence of spatiotemporal Convolution, Normalization,</span>
<span class="sd">    and Activations repeated in the following order:</span>

<span class="sd">    ::</span>

<span class="sd">                                    Conv3d (conv_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                   Activation (act_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                    Conv3d (conv_b)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_b)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                   Activation (act_b)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                    Conv3d (conv_c)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_c)</span>

<span class="sd">    Normalization examples include: BatchNorm3d and None (no normalization).</span>
<span class="sd">    Activation examples include: ReLU, Softmax, Sigmoid, and None (no activation).</span>

<span class="sd">    Args:</span>
<span class="sd">        dim_in (int): input channel size to the bottleneck block.</span>
<span class="sd">        dim_inner (int): intermediate channel size of the bottleneck.</span>
<span class="sd">        dim_out (int): output channel size of the bottleneck.</span>
<span class="sd">        conv_a_kernel_size (tuple): convolutional kernel size(s) for conv_a.</span>
<span class="sd">        conv_a_stride (tuple): convolutional stride size(s) for conv_a.</span>
<span class="sd">        conv_a_padding (tuple): convolutional padding(s) for conv_a.</span>
<span class="sd">        conv_a (callable): a callable that constructs the conv_a conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.</span>
<span class="sd">        conv_b_stride (tuple): convolutional stride size(s) for conv_b.</span>
<span class="sd">        conv_b_padding (tuple): convolutional padding(s) for conv_b.</span>
<span class="sd">        conv_b_num_groups (int): number of groups for groupwise convolution for</span>
<span class="sd">            conv_b.</span>
<span class="sd">        conv_b_dilation (tuple): dilation for 3D convolution for conv_b.</span>
<span class="sd">        conv_b (callable): a callable that constructs the conv_b conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_c (callable): a callable that constructs the conv_c conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>

<span class="sd">        norm (callable): a callable that constructs normalization layer, examples</span>
<span class="sd">            include nn.BatchNorm3d, None (not performing normalization).</span>
<span class="sd">        norm_eps (float): normalization epsilon.</span>
<span class="sd">        norm_momentum (float): normalization momentum.</span>

<span class="sd">        activation (callable): a callable that constructs activation layer, examples</span>
<span class="sd">            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing</span>
<span class="sd">            activation).</span>

<span class="sd">    Returns:</span>
<span class="sd">        (nn.Module): resnet bottleneck block.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">conv_a</span> <span class="o">=</span> <span class="n">conv_a</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_in</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_a_kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">conv_a_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">conv_a_padding</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">norm_a</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">act_a</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation</span><span class="p">()</span>

    <span class="n">conv_b</span> <span class="o">=</span> <span class="n">conv_b</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_b_kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">conv_b_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">conv_b_padding</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">conv_b_num_groups</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">conv_b_dilation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">norm_b</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">act_b</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation</span><span class="p">()</span>

    <span class="n">conv_c</span> <span class="o">=</span> <span class="n">conv_c</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">norm_c</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">BottleneckBlock</span><span class="p">(</span>
        <span class="n">conv_a</span><span class="o">=</span><span class="n">conv_a</span><span class="p">,</span>
        <span class="n">norm_a</span><span class="o">=</span><span class="n">norm_a</span><span class="p">,</span>
        <span class="n">act_a</span><span class="o">=</span><span class="n">act_a</span><span class="p">,</span>
        <span class="n">conv_b</span><span class="o">=</span><span class="n">conv_b</span><span class="p">,</span>
        <span class="n">norm_b</span><span class="o">=</span><span class="n">norm_b</span><span class="p">,</span>
        <span class="n">act_b</span><span class="o">=</span><span class="n">act_b</span><span class="p">,</span>
        <span class="n">conv_c</span><span class="o">=</span><span class="n">conv_c</span><span class="p">,</span>
        <span class="n">norm_c</span><span class="o">=</span><span class="n">norm_c</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="create_acoustic_bottleneck_block"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.create_acoustic_bottleneck_block">[docs]</a><span class="k">def</span> <span class="nf">create_acoustic_bottleneck_block</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Convolution configs.</span>
    <span class="n">dim_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_inner</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">conv_a_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">conv_a</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="c1"># Conv b f configs.</span>
    <span class="n">conv_b_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">conv_b_num_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">conv_b_dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_c</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="c1"># Norm configs.</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
    <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">norm_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># Activation configs.</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Acoustic Bottleneck block: a sequence of spatiotemporal Convolution, Normalization,</span>
<span class="sd">    and Activations repeated in the following order:</span>

<span class="sd">    ::</span>

<span class="sd">                                    Conv3d (conv_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                   Activation (act_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                           ---------------------------------</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                Temporal Conv3d (conv_b)        Spatial Conv3d (conv_b)</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                 Normalization (norm_b)         Normalization (norm_b)</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                   Activation (act_b)              Activation (act_b)</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                           ---------------------------------</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                    Conv3d (conv_c)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_c)</span>

<span class="sd">    Normalization examples include: BatchNorm3d and None (no normalization).</span>
<span class="sd">    Activation examples include: ReLU, Softmax, Sigmoid, and None (no activation).</span>

<span class="sd">    Args:</span>
<span class="sd">        dim_in (int): input channel size to the bottleneck block.</span>
<span class="sd">        dim_inner (int): intermediate channel size of the bottleneck.</span>
<span class="sd">        dim_out (int): output channel size of the bottleneck.</span>
<span class="sd">        conv_a_kernel_size (tuple): convolutional kernel size(s) for conv_a.</span>
<span class="sd">        conv_a_stride (tuple): convolutional stride size(s) for conv_a.</span>
<span class="sd">        conv_a_padding (tuple): convolutional padding(s) for conv_a.</span>
<span class="sd">        conv_a (callable): a callable that constructs the conv_a conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.</span>
<span class="sd">        conv_b_stride (tuple): convolutional stride size(s) for conv_b.</span>
<span class="sd">        conv_b_padding (tuple): convolutional padding(s) for conv_b.</span>
<span class="sd">        conv_b_num_groups (int): number of groups for groupwise convolution for</span>
<span class="sd">            conv_b.</span>
<span class="sd">        conv_b_dilation (tuple): dilation for 3D convolution for conv_b.</span>
<span class="sd">        conv_b (callable): a callable that constructs the conv_b conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_c (callable): a callable that constructs the conv_c conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>

<span class="sd">        norm (callable): a callable that constructs normalization layer, examples</span>
<span class="sd">            include nn.BatchNorm3d, None (not performing normalization).</span>
<span class="sd">        norm_eps (float): normalization epsilon.</span>
<span class="sd">        norm_momentum (float): normalization momentum.</span>

<span class="sd">        activation (callable): a callable that constructs activation layer, examples</span>
<span class="sd">            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing</span>
<span class="sd">            activation).</span>

<span class="sd">    Returns:</span>
<span class="sd">        (nn.Module): resnet acoustic bottleneck block.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">conv_a</span> <span class="o">=</span> <span class="n">conv_a</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_in</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_a_kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">conv_a_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">conv_a_padding</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">norm_a</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">act_a</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation</span><span class="p">()</span>

    <span class="n">conv_b_1_kernel_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_b_kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">conv_b_1_stride</span> <span class="o">=</span> <span class="n">conv_b_stride</span>
    <span class="n">conv_b_1_padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_b_padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="n">conv_b_2_kernel_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">conv_b_kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_b_kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
    <span class="n">conv_b_2_stride</span> <span class="o">=</span> <span class="n">conv_b_stride</span>
    <span class="n">conv_b_2_padding</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">conv_b_padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_b_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>

    <span class="n">conv_b_1_num_groups</span><span class="p">,</span> <span class="n">conv_b_2_num_groups</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_b_num_groups</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">conv_b_1_dilation</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_b_dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">conv_b_2_dilation</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">conv_b_dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_b_dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>

    <span class="n">conv_b_1</span> <span class="o">=</span> <span class="n">conv_b</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_b_1_kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">conv_b_1_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">conv_b_1_padding</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">conv_b_1_num_groups</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">conv_b_1_dilation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">norm_b_1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">act_b_1</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation</span><span class="p">()</span>

    <span class="n">conv_b_2</span> <span class="o">=</span> <span class="n">conv_b</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_b_2_kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">conv_b_2_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">conv_b_2_padding</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">conv_b_2_num_groups</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">conv_b_2_dilation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">norm_b_2</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">act_b_2</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation</span><span class="p">()</span>

    <span class="n">conv_c</span> <span class="o">=</span> <span class="n">conv_c</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">norm_c</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">SeparableBottleneckBlock</span><span class="p">(</span>
        <span class="n">conv_a</span><span class="o">=</span><span class="n">conv_a</span><span class="p">,</span>
        <span class="n">norm_a</span><span class="o">=</span><span class="n">norm_a</span><span class="p">,</span>
        <span class="n">act_a</span><span class="o">=</span><span class="n">act_a</span><span class="p">,</span>
        <span class="n">conv_b</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">conv_b_2</span><span class="p">,</span> <span class="n">conv_b_1</span><span class="p">]),</span>
        <span class="n">norm_b</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">norm_b_2</span><span class="p">,</span> <span class="n">norm_b_1</span><span class="p">]),</span>
        <span class="n">act_b</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">act_b_2</span><span class="p">,</span> <span class="n">act_b_1</span><span class="p">]),</span>
        <span class="n">conv_c</span><span class="o">=</span><span class="n">conv_c</span><span class="p">,</span>
        <span class="n">norm_c</span><span class="o">=</span><span class="n">norm_c</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="create_res_block"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.create_res_block">[docs]</a><span class="k">def</span> <span class="nf">create_res_block</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Bottleneck Block configs.</span>
    <span class="n">dim_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_inner</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">bottleneck</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">use_shortcut</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">branch_fusion</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span>
    <span class="c1"># Conv configs.</span>
    <span class="n">conv_a_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">conv_a</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_b_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">conv_b_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">conv_b_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b_num_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">conv_b_dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_c</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_skip</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="c1"># Norm configs.</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
    <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">norm_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># Activation configs.</span>
    <span class="n">activation_bottleneck</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="n">activation_block</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Residual block. Performs a summation between an identity shortcut in branch1 and a</span>
<span class="sd">    main block in branch2. When the input and output dimensions are different, a</span>
<span class="sd">    convolution followed by a normalization will be performed.</span>

<span class="sd">    ::</span>


<span class="sd">                                         Input</span>
<span class="sd">                                           |-------+</span>
<span class="sd">                                           ↓       |</span>
<span class="sd">                                         Block     |</span>
<span class="sd">                                           ↓       |</span>
<span class="sd">                                       Summation ←-+</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                       Activation</span>

<span class="sd">    Normalization examples include: BatchNorm3d and None (no normalization).</span>
<span class="sd">    Activation examples include: ReLU, Softmax, Sigmoid, and None (no activation).</span>
<span class="sd">    Transform examples include: BottleneckBlock.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim_in (int): input channel size to the bottleneck block.</span>
<span class="sd">        dim_inner (int): intermediate channel size of the bottleneck.</span>
<span class="sd">        dim_out (int): output channel size of the bottleneck.</span>
<span class="sd">        bottleneck (callable): a callable that constructs bottleneck block layer.</span>
<span class="sd">            Examples include: create_bottleneck_block.</span>
<span class="sd">        use_shortcut (bool): If true, use conv and norm layers in skip connection.</span>
<span class="sd">        branch_fusion (callable): a callable that constructs summation layer.</span>
<span class="sd">            Examples include: lambda x, y: x + y, OctaveSum.</span>

<span class="sd">        conv_a_kernel_size (tuple): convolutional kernel size(s) for conv_a.</span>
<span class="sd">        conv_a_stride (tuple): convolutional stride size(s) for conv_a.</span>
<span class="sd">        conv_a_padding (tuple): convolutional padding(s) for conv_a.</span>
<span class="sd">        conv_a (callable): a callable that constructs the conv_a conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.</span>
<span class="sd">        conv_b_stride (tuple): convolutional stride size(s) for conv_b.</span>
<span class="sd">        conv_b_padding (tuple): convolutional padding(s) for conv_b.</span>
<span class="sd">        conv_b_num_groups (int): number of groups for groupwise convolution for</span>
<span class="sd">            conv_b.</span>
<span class="sd">        conv_b_dilation (tuple): dilation for 3D convolution for conv_b.</span>
<span class="sd">        conv_b (callable): a callable that constructs the conv_b conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_c (callable): a callable that constructs the conv_c conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_skip (callable): a callable that constructs the conv_skip conv layer,</span>
<span class="sd">        examples include nn.Conv3d, OctaveConv, etc</span>

<span class="sd">        norm (callable): a callable that constructs normalization layer. Examples</span>
<span class="sd">            include nn.BatchNorm3d, None (not performing normalization).</span>
<span class="sd">        norm_eps (float): normalization epsilon.</span>
<span class="sd">        norm_momentum (float): normalization momentum.</span>

<span class="sd">        activation_bottleneck (callable): a callable that constructs activation layer in</span>
<span class="sd">            bottleneck. Examples include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None</span>
<span class="sd">            (not performing activation).</span>
<span class="sd">        activation_block (callable): a callable that constructs activation layer used</span>
<span class="sd">            at the end of the block. Examples include: nn.ReLU, nn.Softmax, nn.Sigmoid,</span>
<span class="sd">            and None (not performing activation).</span>

<span class="sd">    Returns:</span>
<span class="sd">        (nn.Module): resnet basic block layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">branch1_conv_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">conv_a_stride</span><span class="p">,</span> <span class="n">conv_b_stride</span><span class="p">)))</span>
    <span class="n">norm_model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">use_shortcut</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dim_in</span> <span class="o">!=</span> <span class="n">dim_out</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">branch1_conv_stride</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="n">norm_model</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ResBlock</span><span class="p">(</span>
        <span class="n">branch1_conv</span><span class="o">=</span><span class="n">conv_skip</span><span class="p">(</span>
            <span class="n">dim_in</span><span class="p">,</span>
            <span class="n">dim_out</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">branch1_conv_stride</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">dim_in</span> <span class="o">!=</span> <span class="n">dim_out</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">branch1_conv_stride</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="n">use_shortcut</span>
        <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch1_norm</span><span class="o">=</span><span class="n">norm_model</span><span class="p">,</span>
        <span class="n">branch2</span><span class="o">=</span><span class="n">bottleneck</span><span class="p">(</span>
            <span class="n">dim_in</span><span class="o">=</span><span class="n">dim_in</span><span class="p">,</span>
            <span class="n">dim_inner</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
            <span class="n">dim_out</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span>
            <span class="n">conv_a_kernel_size</span><span class="o">=</span><span class="n">conv_a_kernel_size</span><span class="p">,</span>
            <span class="n">conv_a_stride</span><span class="o">=</span><span class="n">conv_a_stride</span><span class="p">,</span>
            <span class="n">conv_a_padding</span><span class="o">=</span><span class="n">conv_a_padding</span><span class="p">,</span>
            <span class="n">conv_a</span><span class="o">=</span><span class="n">conv_a</span><span class="p">,</span>
            <span class="n">conv_b_kernel_size</span><span class="o">=</span><span class="n">conv_b_kernel_size</span><span class="p">,</span>
            <span class="n">conv_b_stride</span><span class="o">=</span><span class="n">conv_b_stride</span><span class="p">,</span>
            <span class="n">conv_b_padding</span><span class="o">=</span><span class="n">conv_b_padding</span><span class="p">,</span>
            <span class="n">conv_b_num_groups</span><span class="o">=</span><span class="n">conv_b_num_groups</span><span class="p">,</span>
            <span class="n">conv_b_dilation</span><span class="o">=</span><span class="n">conv_b_dilation</span><span class="p">,</span>
            <span class="n">conv_b</span><span class="o">=</span><span class="n">conv_b</span><span class="p">,</span>
            <span class="n">conv_c</span><span class="o">=</span><span class="n">conv_c</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
            <span class="n">norm_eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span>
            <span class="n">norm_momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation_bottleneck</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">activation</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">activation_block</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation_block</span><span class="p">(),</span>
        <span class="n">branch_fusion</span><span class="o">=</span><span class="n">branch_fusion</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="create_res_stage"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.create_res_stage">[docs]</a><span class="k">def</span> <span class="nf">create_res_stage</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Stage configs.</span>
    <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="c1"># Bottleneck Block configs.</span>
    <span class="n">dim_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_inner</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">bottleneck</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="c1"># Conv configs.</span>
    <span class="n">conv_a_kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_a_padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">conv_a</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_b_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">conv_b_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">conv_b_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b_num_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">conv_b_dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_c</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="c1"># Norm configs.</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
    <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">norm_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># Activation configs.</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create Residual Stage, which composes sequential blocks that make up a ResNet. These</span>
<span class="sd">    blocks could be, for example, Residual blocks, Non-Local layers, or</span>
<span class="sd">    Squeeze-Excitation layers.</span>

<span class="sd">    ::</span>


<span class="sd">                                        Input</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                       ResBlock</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                       ResBlock</span>

<span class="sd">    Normalization examples include: BatchNorm3d and None (no normalization).</span>
<span class="sd">    Activation examples include: ReLU, Softmax, Sigmoid, and None (no activation).</span>
<span class="sd">    Bottleneck examples include: create_bottleneck_block.</span>

<span class="sd">    Args:</span>
<span class="sd">        depth (init): number of blocks to create.</span>

<span class="sd">        dim_in (int): input channel size to the bottleneck block.</span>
<span class="sd">        dim_inner (int): intermediate channel size of the bottleneck.</span>
<span class="sd">        dim_out (int): output channel size of the bottleneck.</span>
<span class="sd">        bottleneck (callable): a callable that constructs bottleneck block layer.</span>
<span class="sd">            Examples include: create_bottleneck_block.</span>

<span class="sd">        conv_a_kernel_size (tuple or list of tuple): convolutional kernel size(s)</span>
<span class="sd">            for conv_a. If conv_a_kernel_size is a tuple, use it for all blocks in</span>
<span class="sd">            the stage. If conv_a_kernel_size is a list of tuple, the kernel sizes</span>
<span class="sd">            will be repeated until having same length of depth in the stage. For</span>
<span class="sd">            example, for conv_a_kernel_size = [(3, 1, 1), (1, 1, 1)], the kernel</span>
<span class="sd">            size for the first 6 blocks would be [(3, 1, 1), (1, 1, 1), (3, 1, 1),</span>
<span class="sd">            (1, 1, 1), (3, 1, 1)].</span>
<span class="sd">        conv_a_stride (tuple): convolutional stride size(s) for conv_a.</span>
<span class="sd">        conv_a_padding (tuple or list of tuple): convolutional padding(s) for</span>
<span class="sd">            conv_a. If conv_a_padding is a tuple, use it for all blocks in</span>
<span class="sd">            the stage. If conv_a_padding is a list of tuple, the padding sizes</span>
<span class="sd">            will be repeated until having same length of depth in the stage.</span>
<span class="sd">        conv_a (callable): a callable that constructs the conv_a conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.</span>
<span class="sd">        conv_b_stride (tuple): convolutional stride size(s) for conv_b.</span>
<span class="sd">        conv_b_padding (tuple): convolutional padding(s) for conv_b.</span>
<span class="sd">        conv_b_num_groups (int): number of groups for groupwise convolution for</span>
<span class="sd">            conv_b.</span>
<span class="sd">        conv_b_dilation (tuple): dilation for 3D convolution for conv_b.</span>
<span class="sd">        conv_b (callable): a callable that constructs the conv_b conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_c (callable): a callable that constructs the conv_c conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>

<span class="sd">        norm (callable): a callable that constructs normalization layer. Examples</span>
<span class="sd">            include nn.BatchNorm3d, and None (not performing normalization).</span>
<span class="sd">        norm_eps (float): normalization epsilon.</span>
<span class="sd">        norm_momentum (float): normalization momentum.</span>

<span class="sd">        activation (callable): a callable that constructs activation layer. Examples</span>
<span class="sd">            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing</span>
<span class="sd">            activation).</span>

<span class="sd">    Returns:</span>
<span class="sd">        (nn.Module): resnet basic stage layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">res_blocks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">conv_a_kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">conv_a_kernel_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_a_kernel_size</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">conv_a_padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">conv_a_padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_a_padding</span><span class="p">]</span>
    <span class="c1"># Repeat conv_a kernels until having same length of depth in the stage.</span>
    <span class="n">conv_a_kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_a_kernel_size</span> <span class="o">*</span> <span class="n">depth</span><span class="p">)[:</span><span class="n">depth</span><span class="p">]</span>
    <span class="n">conv_a_padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_a_padding</span> <span class="o">*</span> <span class="n">depth</span><span class="p">)[:</span><span class="n">depth</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
        <span class="n">block</span> <span class="o">=</span> <span class="n">create_res_block</span><span class="p">(</span>
            <span class="n">dim_in</span><span class="o">=</span><span class="n">dim_in</span> <span class="k">if</span> <span class="n">ind</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim_out</span><span class="p">,</span>
            <span class="n">dim_inner</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
            <span class="n">dim_out</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span>
            <span class="n">bottleneck</span><span class="o">=</span><span class="n">bottleneck</span><span class="p">,</span>
            <span class="n">conv_a_kernel_size</span><span class="o">=</span><span class="n">conv_a_kernel_size</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span>
            <span class="n">conv_a_stride</span><span class="o">=</span><span class="n">conv_a_stride</span> <span class="k">if</span> <span class="n">ind</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">conv_a_padding</span><span class="o">=</span><span class="n">conv_a_padding</span><span class="p">[</span><span class="n">ind</span><span class="p">],</span>
            <span class="n">conv_a</span><span class="o">=</span><span class="n">conv_a</span><span class="p">,</span>
            <span class="n">conv_b_kernel_size</span><span class="o">=</span><span class="n">conv_b_kernel_size</span><span class="p">,</span>
            <span class="n">conv_b_stride</span><span class="o">=</span><span class="n">conv_b_stride</span> <span class="k">if</span> <span class="n">ind</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">conv_b_padding</span><span class="o">=</span><span class="n">conv_b_padding</span><span class="p">,</span>
            <span class="n">conv_b_num_groups</span><span class="o">=</span><span class="n">conv_b_num_groups</span><span class="p">,</span>
            <span class="n">conv_b_dilation</span><span class="o">=</span><span class="n">conv_b_dilation</span><span class="p">,</span>
            <span class="n">conv_b</span><span class="o">=</span><span class="n">conv_b</span><span class="p">,</span>
            <span class="n">conv_c</span><span class="o">=</span><span class="n">conv_c</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
            <span class="n">norm_eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span>
            <span class="n">norm_momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">,</span>
            <span class="n">activation_bottleneck</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">activation_block</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">res_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ResStage</span><span class="p">(</span><span class="n">res_blocks</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">res_blocks</span><span class="p">))</span></div>


<div class="viewcode-block" id="create_resnet"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.create_resnet">[docs]</a><span class="k">def</span> <span class="nf">create_resnet</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Input clip configs.</span>
    <span class="n">input_channel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="c1"># Model configs.</span>
    <span class="n">model_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">model_num_class</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="c1"># Normalization configs.</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
    <span class="c1"># Activation configs.</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="c1"># Stem configs.</span>
    <span class="n">stem_dim_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">stem_conv_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
    <span class="n">stem_conv_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">stem_pool</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">,</span>
    <span class="n">stem_pool_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">stem_pool_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="c1"># Stage configs.</span>
    <span class="n">stage1_pool</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stage1_pool_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stage_conv_a_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">),</span>
    <span class="n">stage_conv_b_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">),</span>
    <span class="n">stage_conv_b_num_groups</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stage_conv_b_dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">),</span>
    <span class="n">stage_spatial_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">stage_temporal_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">bottleneck</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">create_bottleneck_block</span><span class="p">,</span>
    <span class="c1"># Head configs.</span>
    <span class="n">head_pool</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">,</span>
    <span class="n">head_pool_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
    <span class="n">head_output_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_output_with_global_average</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build ResNet style models for video recognition. ResNet has three parts:</span>
<span class="sd">    Stem, Stages and Head. Stem is the first Convolution layer (Conv1) with an</span>
<span class="sd">    optional pooling layer. Stages are grouped residual blocks. There are usually</span>
<span class="sd">    multiple stages and each stage may include multiple residual blocks. Head</span>
<span class="sd">    may include pooling, dropout, a fully-connected layer and global spatial</span>
<span class="sd">    temporal averaging. The three parts are assembled in the following order:</span>

<span class="sd">    ::</span>

<span class="sd">                                         Input</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Stem</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Stage 1</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Stage N</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Head</span>

<span class="sd">    Args:</span>

<span class="sd">        input_channel (int): number of channels for the input video clip.</span>

<span class="sd">        model_depth (int): the depth of the resnet. Options include: 50, 101, 152.</span>
<span class="sd">        model_num_class (int): the number of classes for the video dataset.</span>
<span class="sd">        dropout_rate (float): dropout rate.</span>


<span class="sd">        norm (callable): a callable that constructs normalization layer.</span>

<span class="sd">        activation (callable): a callable that constructs activation layer.</span>

<span class="sd">        stem_dim_out (int): output channel size to stem.</span>
<span class="sd">        stem_conv_kernel_size (tuple): convolutional kernel size(s) of stem.</span>
<span class="sd">        stem_conv_stride (tuple): convolutional stride size(s) of stem.</span>
<span class="sd">        stem_pool (callable): a callable that constructs resnet head pooling layer.</span>
<span class="sd">        stem_pool_kernel_size (tuple): pooling kernel size(s).</span>
<span class="sd">        stem_pool_stride (tuple): pooling stride size(s).</span>

<span class="sd">        stage_conv_a_kernel_size (tuple): convolutional kernel size(s) for conv_a.</span>
<span class="sd">        stage_conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.</span>
<span class="sd">        stage_conv_b_num_groups (tuple): number of groups for groupwise convolution</span>
<span class="sd">            for conv_b. 1 for ResNet, and larger than 1 for ResNeXt.</span>
<span class="sd">        stage_conv_b_dilation (tuple): dilation for 3D convolution for conv_b.</span>
<span class="sd">        stage_spatial_stride (tuple): the spatial stride for each stage.</span>
<span class="sd">        stage_temporal_stride (tuple): the temporal stride for each stage.</span>
<span class="sd">        bottleneck (callable): a callable that constructs bottleneck block layer.</span>
<span class="sd">            Examples include: create_bottleneck_block.</span>

<span class="sd">        head_pool (callable): a callable that constructs resnet head pooling layer.</span>
<span class="sd">        head_pool_kernel_size (tuple): the pooling kernel size.</span>
<span class="sd">        head_output_size (tuple): the size of output tensor for head.</span>
<span class="sd">        head_activation (callable): a callable that constructs activation layer.</span>
<span class="sd">        head_output_with_global_average (bool): if True, perform global averaging on</span>
<span class="sd">            the head output.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (nn.Module): basic resnet.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Number of blocks for different stages given the model depth.</span>
    <span class="n">_MODEL_STAGE_DEPTH</span> <span class="o">=</span> <span class="p">{</span><span class="mi">50</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">101</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">152</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>

    <span class="c1"># Given a model depth, get the number of blocks for each stage.</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">model_depth</span> <span class="ow">in</span> <span class="n">_MODEL_STAGE_DEPTH</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_depth</span><span class="si">}</span><span class="s2"> is not in </span><span class="si">{</span><span class="n">_MODEL_STAGE_DEPTH</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">stage_depths</span> <span class="o">=</span> <span class="n">_MODEL_STAGE_DEPTH</span><span class="p">[</span><span class="n">model_depth</span><span class="p">]</span>

    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Create stem for resnet.</span>
    <span class="n">stem</span> <span class="o">=</span> <span class="n">create_res_basic_stem</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">input_channel</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">stem_dim_out</span><span class="p">,</span>
        <span class="n">conv_kernel_size</span><span class="o">=</span><span class="n">stem_conv_kernel_size</span><span class="p">,</span>
        <span class="n">conv_stride</span><span class="o">=</span><span class="n">stem_conv_stride</span><span class="p">,</span>
        <span class="n">conv_padding</span><span class="o">=</span><span class="p">[</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">stem_conv_kernel_size</span><span class="p">],</span>
        <span class="n">pool</span><span class="o">=</span><span class="n">stem_pool</span><span class="p">,</span>
        <span class="n">pool_kernel_size</span><span class="o">=</span><span class="n">stem_pool_kernel_size</span><span class="p">,</span>
        <span class="n">pool_stride</span><span class="o">=</span><span class="n">stem_pool_stride</span><span class="p">,</span>
        <span class="n">pool_padding</span><span class="o">=</span><span class="p">[</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">stem_pool_kernel_size</span><span class="p">],</span>
        <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stem</span><span class="p">)</span>

    <span class="n">stage_dim_in</span> <span class="o">=</span> <span class="n">stem_dim_out</span>
    <span class="n">stage_dim_out</span> <span class="o">=</span> <span class="n">stage_dim_in</span> <span class="o">*</span> <span class="mi">4</span>

    <span class="c1"># Create each stage for resnet.</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stage_depths</span><span class="p">)):</span>
        <span class="n">stage_dim_inner</span> <span class="o">=</span> <span class="n">stage_dim_out</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">depth</span> <span class="o">=</span> <span class="n">stage_depths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">stage_conv_a_kernel</span> <span class="o">=</span> <span class="n">stage_conv_a_kernel_size</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">stage_conv_a_stride</span> <span class="o">=</span> <span class="p">(</span><span class="n">stage_temporal_stride</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">stage_conv_a_padding</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">stage_conv_a_kernel</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stage_conv_a_kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">[[</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">]</span> <span class="k">for</span> <span class="n">sizes</span> <span class="ow">in</span> <span class="n">stage_conv_a_kernel</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">stage_conv_b_stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stage_spatial_stride</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">stage_spatial_stride</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

        <span class="n">stage</span> <span class="o">=</span> <span class="n">create_res_stage</span><span class="p">(</span>
            <span class="n">depth</span><span class="o">=</span><span class="n">depth</span><span class="p">,</span>
            <span class="n">dim_in</span><span class="o">=</span><span class="n">stage_dim_in</span><span class="p">,</span>
            <span class="n">dim_inner</span><span class="o">=</span><span class="n">stage_dim_inner</span><span class="p">,</span>
            <span class="n">dim_out</span><span class="o">=</span><span class="n">stage_dim_out</span><span class="p">,</span>
            <span class="n">bottleneck</span><span class="o">=</span><span class="n">bottleneck</span><span class="p">,</span>
            <span class="n">conv_a_kernel_size</span><span class="o">=</span><span class="n">stage_conv_a_kernel</span><span class="p">,</span>
            <span class="n">conv_a_stride</span><span class="o">=</span><span class="n">stage_conv_a_stride</span><span class="p">,</span>
            <span class="n">conv_a_padding</span><span class="o">=</span><span class="n">stage_conv_a_padding</span><span class="p">,</span>
            <span class="n">conv_b_kernel_size</span><span class="o">=</span><span class="n">stage_conv_b_kernel_size</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
            <span class="n">conv_b_stride</span><span class="o">=</span><span class="n">stage_conv_b_stride</span><span class="p">,</span>
            <span class="n">conv_b_padding</span><span class="o">=</span><span class="p">[</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">stage_conv_b_kernel_size</span><span class="p">[</span><span class="n">idx</span><span class="p">]],</span>
            <span class="n">conv_b_num_groups</span><span class="o">=</span><span class="n">stage_conv_b_num_groups</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
            <span class="n">conv_b_dilation</span><span class="o">=</span><span class="n">stage_conv_b_dilation</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
        <span class="n">stage_dim_in</span> <span class="o">=</span> <span class="n">stage_dim_out</span>
        <span class="n">stage_dim_out</span> <span class="o">=</span> <span class="n">stage_dim_out</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">stage1_pool</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">stage1_pool</span><span class="p">(</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="n">stage1_pool_kernel_size</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="n">stage1_pool_kernel_size</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="n">head</span> <span class="o">=</span> <span class="n">create_res_basic_head</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="n">stage_dim_in</span><span class="p">,</span>
        <span class="n">out_features</span><span class="o">=</span><span class="n">model_num_class</span><span class="p">,</span>
        <span class="n">pool</span><span class="o">=</span><span class="n">head_pool</span><span class="p">,</span>
        <span class="n">output_size</span><span class="o">=</span><span class="n">head_output_size</span><span class="p">,</span>
        <span class="n">pool_kernel_size</span><span class="o">=</span><span class="n">head_pool_kernel_size</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">head_activation</span><span class="p">,</span>
        <span class="n">output_with_global_average</span><span class="o">=</span><span class="n">head_output_with_global_average</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">head</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Net</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">blocks</span><span class="p">))</span></div>


<div class="viewcode-block" id="create_acoustic_building_block"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.create_acoustic_building_block">[docs]</a><span class="k">def</span> <span class="nf">create_acoustic_building_block</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Convolution configs.</span>
    <span class="n">dim_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_inner</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dim_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">conv_a_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">conv_a_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">conv_a_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">conv_a</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># Conv b f configs.</span>
    <span class="n">conv_b_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">conv_b_num_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">conv_b_dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">conv_b</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="n">conv_c</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">,</span>
    <span class="c1"># Norm configs.</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
    <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">norm_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># Activation configs.</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Acoustic building block: a sequence of spatiotemporal Convolution, Normalization,</span>
<span class="sd">    and Activations repeated in the following order:</span>

<span class="sd">    ::</span>


<span class="sd">                                    Conv3d (conv_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                   Activation (act_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                           ---------------------------------</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                Temporal Conv3d (conv_b)        Spatial Conv3d (conv_b)</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                 Normalization (norm_b)         Normalization (norm_b)</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                   Activation (act_b)              Activation (act_b)</span>
<span class="sd">                           ↓                               ↓</span>
<span class="sd">                           ---------------------------------</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                    Conv3d (conv_c)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_c)</span>

<span class="sd">    Normalization examples include: BatchNorm3d and None (no normalization).</span>
<span class="sd">    Activation examples include: ReLU, Softmax, Sigmoid, and None (no activation).</span>

<span class="sd">    Args:</span>

<span class="sd">        dim_in (int): input channel size to the bottleneck block.</span>
<span class="sd">        dim_inner (int): intermediate channel size of the bottleneck.</span>
<span class="sd">        dim_out (int): output channel size of the bottleneck.</span>
<span class="sd">        conv_a_kernel_size (tuple): convolutional kernel size(s) for conv_a.</span>
<span class="sd">        conv_a_stride (tuple): convolutional stride size(s) for conv_a.</span>
<span class="sd">        conv_a_padding (tuple): convolutional padding(s) for conv_a.</span>
<span class="sd">        conv_a (callable): a callable that constructs the conv_a conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.</span>
<span class="sd">        conv_b_stride (tuple): convolutional stride size(s) for conv_b.</span>
<span class="sd">        conv_b_padding (tuple): convolutional padding(s) for conv_b.</span>
<span class="sd">        conv_b_num_groups (int): number of groups for groupwise convolution for</span>
<span class="sd">            conv_b.</span>
<span class="sd">        conv_b_dilation (tuple): dilation for 3D convolution for conv_b.</span>
<span class="sd">        conv_b (callable): a callable that constructs the conv_b conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>
<span class="sd">        conv_c (callable): a callable that constructs the conv_c conv layer, examples</span>
<span class="sd">            include nn.Conv3d, OctaveConv, etc</span>

<span class="sd">        norm (callable): a callable that constructs normalization layer, examples</span>
<span class="sd">            include nn.BatchNorm3d, None (not performing normalization).</span>
<span class="sd">        norm_eps (float): normalization epsilon.</span>
<span class="sd">        norm_momentum (float): normalization momentum.</span>

<span class="sd">        activation (callable): a callable that constructs activation layer, examples</span>
<span class="sd">            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing</span>
<span class="sd">            activation).</span>

<span class="sd">    Returns:</span>
<span class="sd">        (nn.Module): resnet acoustic bottleneck block.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Conv b f configs.</span>
    <span class="n">conv_b_1_kernel_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_b_kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">conv_b_2_kernel_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">conv_b_kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_b_kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>

    <span class="n">conv_b_1_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_b_stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">conv_b_2_stride</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">conv_b_stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_b_stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>

    <span class="n">conv_b_1_padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_b_padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">conv_b_2_padding</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">conv_b_padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_b_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>

    <span class="n">conv_b_1_num_groups</span><span class="p">,</span> <span class="n">conv_b_2_num_groups</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_b_num_groups</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">2</span>

    <span class="n">conv_b_1_dilation</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_b_dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">conv_b_2_dilation</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">conv_b_dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">conv_b_dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>

    <span class="n">conv_b_1</span> <span class="o">=</span> <span class="n">conv_b</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_in</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_b_1_kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">conv_b_1_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">conv_b_1_padding</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">conv_b_1_num_groups</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">conv_b_1_dilation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">norm_b_1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">act_b_1</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation</span><span class="p">()</span>

    <span class="n">conv_b_2</span> <span class="o">=</span> <span class="n">conv_b</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_in</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_b_2_kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">conv_b_2_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">conv_b_2_padding</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">conv_b_2_num_groups</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">conv_b_2_dilation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">norm_b_2</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">act_b_2</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">activation</span><span class="p">()</span>

    <span class="n">conv_c</span> <span class="o">=</span> <span class="n">conv_c</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">dim_inner</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">norm_c</span> <span class="o">=</span> <span class="p">(</span>
        <span class="kc">None</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">norm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">norm_momentum</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">SeparableBottleneckBlock</span><span class="p">(</span>
        <span class="n">conv_a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">act_a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_b</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">conv_b_1</span><span class="p">,</span> <span class="n">conv_b_2</span><span class="p">]),</span>
        <span class="n">norm_b</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">norm_b_1</span><span class="p">,</span> <span class="n">norm_b_2</span><span class="p">]),</span>
        <span class="n">act_b</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">act_b_1</span><span class="p">,</span> <span class="n">act_b_2</span><span class="p">]),</span>
        <span class="n">conv_c</span><span class="o">=</span><span class="n">conv_c</span><span class="p">,</span>
        <span class="n">norm_c</span><span class="o">=</span><span class="n">norm_c</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="create_acoustic_resnet"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.create_acoustic_resnet">[docs]</a><span class="k">def</span> <span class="nf">create_acoustic_resnet</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Model configs.</span>
    <span class="n">input_channel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">model_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">model_num_class</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="c1"># Normalization configs.</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
    <span class="c1"># Activation configs.</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="c1"># Stem configs.</span>
    <span class="n">stem_dim_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">stem_conv_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
    <span class="n">stem_conv_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stem_conv_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">stem_pool</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">,</span>
    <span class="n">stem_pool_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">stem_pool_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">stem</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">create_acoustic_res_basic_stem</span><span class="p">,</span>
    <span class="c1"># Stage configs.</span>
    <span class="n">stage_conv_a_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stage_conv_a_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">stage_conv_b_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">stage_conv_b_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stage_conv_b_num_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">stage_conv_b_dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stage_spatial_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">stage_temporal_stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">bottleneck</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">create_acoustic_bottleneck_block</span><span class="p">,</span>
        <span class="n">create_acoustic_bottleneck_block</span><span class="p">,</span>
        <span class="n">create_bottleneck_block</span><span class="p">,</span>
        <span class="n">create_bottleneck_block</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># Head configs.</span>
    <span class="n">head_pool</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">,</span>
    <span class="n">head_output_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">head_activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">,</span>
    <span class="n">head_pool_kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build ResNet style models for acoustic recognition. ResNet has three parts:</span>
<span class="sd">    Stem, Stages and Head. The three parts are assembled in the following order:</span>

<span class="sd">    ::</span>

<span class="sd">                                         Input</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Stem</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Stage 1</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Stage N</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                         Head</span>

<span class="sd">    Args:</span>

<span class="sd">        input_channel (int): number of channels for the input video clip.</span>
<span class="sd">        input_clip_length (int): length of the input video clip.</span>
<span class="sd">        input_crop_size (int): spatial resolution of the input video clip.</span>

<span class="sd">        model_depth (int): the depth of the resnet.</span>
<span class="sd">        model_num_class (int): the number of classes for the video dataset.</span>
<span class="sd">        dropout_rate (float): dropout rate.</span>

<span class="sd">        norm (callable): a callable that constructs normalization layer.</span>

<span class="sd">        activation (callable): a callable that constructs activation layer.</span>

<span class="sd">        stem_dim_out (int): output channel size to stem.</span>
<span class="sd">        stem_conv_kernel_size (tuple): convolutional kernel size(s) of stem.</span>
<span class="sd">        stem_conv_stride (tuple): convolutional stride size(s) of stem.</span>
<span class="sd">        stem_pool (callable): a callable that constructs resnet head pooling layer.</span>
<span class="sd">        stem_pool_kernel_size (tuple): pooling kernel size(s).</span>
<span class="sd">        stem_pool_stride (tuple): pooling stride size(s).</span>
<span class="sd">        stem (callable): a callable that constructs stem layer.</span>
<span class="sd">            Examples include: create_res_video_stem.</span>

<span class="sd">        stage_conv_a_kernel_size (tuple): convolutional kernel size(s) for conv_a.</span>
<span class="sd">        stage_conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.</span>
<span class="sd">        stage_conv_b_num_groups (int): number of groups for groupwise convolution</span>
<span class="sd">            for conv_b. 1 for ResNet, and larger than 1 for ResNeXt.</span>
<span class="sd">        stage_conv_b_dilation (tuple): dilation for 3D convolution for conv_b.</span>
<span class="sd">        stage_spatial_stride (tuple): the spatial stride for each stage.</span>
<span class="sd">        stage_temporal_stride (tuple): the temporal stride for each stage.</span>
<span class="sd">        bottleneck (callable): a callable that constructs bottleneck block</span>
<span class="sd">            layer.</span>
<span class="sd">            Examples include: create_bottleneck_block.</span>

<span class="sd">        head_pool (callable): a callable that constructs resnet head pooling layer.</span>
<span class="sd">        head_output_size (tuple): the size of output tensor for head.</span>
<span class="sd">        head_activation (callable): a callable that constructs activation layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (nn.Module): acoustic resnet that takes audio inputs in log-mel-spectrogram of</span>
<span class="sd">            shape B x 1 x 1 x T x F.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Given a model depth, get the number of blocks for each stage.</span>
    <span class="n">_MODEL_STAGE_DEPTH</span> <span class="o">=</span> <span class="p">{</span><span class="mi">50</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">101</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">152</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>
    <span class="k">assert</span> <span class="n">model_depth</span> <span class="ow">in</span> <span class="n">_MODEL_STAGE_DEPTH</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="n">stage_depths</span> <span class="o">=</span> <span class="n">_MODEL_STAGE_DEPTH</span><span class="p">[</span><span class="n">model_depth</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">stage_depths</span><span class="p">)</span>

    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Create stem for resnet.</span>
    <span class="n">stem</span> <span class="o">=</span> <span class="n">stem</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">input_channel</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">stem_dim_out</span><span class="p">,</span>
        <span class="n">conv_kernel_size</span><span class="o">=</span><span class="n">stem_conv_kernel_size</span><span class="p">,</span>
        <span class="n">conv_stride</span><span class="o">=</span><span class="n">stem_conv_stride</span><span class="p">,</span>
        <span class="n">conv_padding</span><span class="o">=</span><span class="n">stem_conv_padding</span><span class="p">,</span>
        <span class="n">pool</span><span class="o">=</span><span class="n">stem_pool</span><span class="p">,</span>
        <span class="n">pool_kernel_size</span><span class="o">=</span><span class="n">stem_pool_kernel_size</span><span class="p">,</span>
        <span class="n">pool_stride</span><span class="o">=</span><span class="n">stem_pool_stride</span><span class="p">,</span>
        <span class="n">pool_padding</span><span class="o">=</span><span class="p">[</span><span class="n">size</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">stem_pool_kernel_size</span><span class="p">],</span>
        <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stem</span><span class="p">)</span>
    <span class="n">stage_dim_in</span> <span class="o">=</span> <span class="n">stem_dim_out</span>
    <span class="n">stage_dim_out</span> <span class="o">=</span> <span class="n">stage_dim_in</span> <span class="o">*</span> <span class="mi">4</span>

    <span class="c1"># Create each stage for resnet.</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stage_depths</span><span class="p">)):</span>
        <span class="n">stage_dim_inner</span> <span class="o">=</span> <span class="n">stage_dim_out</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">depth</span> <span class="o">=</span> <span class="n">stage_depths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="n">stage_conv_a_stride</span> <span class="o">=</span> <span class="p">(</span><span class="n">stage_temporal_stride</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">stage_conv_b_stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stage_spatial_stride</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">stage_spatial_stride</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

        <span class="n">stage</span> <span class="o">=</span> <span class="n">create_res_stage</span><span class="p">(</span>
            <span class="n">depth</span><span class="o">=</span><span class="n">depth</span><span class="p">,</span>
            <span class="n">dim_in</span><span class="o">=</span><span class="n">stage_dim_in</span><span class="p">,</span>
            <span class="n">dim_inner</span><span class="o">=</span><span class="n">stage_dim_inner</span><span class="p">,</span>
            <span class="n">dim_out</span><span class="o">=</span><span class="n">stage_dim_out</span><span class="p">,</span>
            <span class="n">bottleneck</span><span class="o">=</span><span class="n">bottleneck</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span>
            <span class="n">conv_a_kernel_size</span><span class="o">=</span><span class="n">stage_conv_a_kernel_size</span><span class="p">,</span>
            <span class="n">conv_a_stride</span><span class="o">=</span><span class="n">stage_conv_a_stride</span><span class="p">,</span>
            <span class="n">conv_a_padding</span><span class="o">=</span><span class="n">stage_conv_a_padding</span><span class="p">,</span>
            <span class="n">conv_b_kernel_size</span><span class="o">=</span><span class="n">stage_conv_b_kernel_size</span><span class="p">,</span>
            <span class="n">conv_b_stride</span><span class="o">=</span><span class="n">stage_conv_b_stride</span><span class="p">,</span>
            <span class="n">conv_b_padding</span><span class="o">=</span><span class="n">stage_conv_b_padding</span><span class="p">,</span>
            <span class="n">conv_b_num_groups</span><span class="o">=</span><span class="n">stage_conv_b_num_groups</span><span class="p">,</span>
            <span class="n">conv_b_dilation</span><span class="o">=</span><span class="n">stage_conv_b_dilation</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
        <span class="n">stage_dim_in</span> <span class="o">=</span> <span class="n">stage_dim_out</span>
        <span class="n">stage_dim_out</span> <span class="o">=</span> <span class="n">stage_dim_out</span> <span class="o">*</span> <span class="mi">2</span>

    <span class="c1"># Create head for resnet.</span>
    <span class="n">head</span> <span class="o">=</span> <span class="n">create_res_basic_head</span><span class="p">(</span>
        <span class="n">in_features</span><span class="o">=</span><span class="n">stage_dim_in</span><span class="p">,</span>
        <span class="n">out_features</span><span class="o">=</span><span class="n">model_num_class</span><span class="p">,</span>
        <span class="n">pool</span><span class="o">=</span><span class="n">head_pool</span><span class="p">,</span>
        <span class="n">output_size</span><span class="o">=</span><span class="n">head_output_size</span><span class="p">,</span>
        <span class="n">pool_kernel_size</span><span class="o">=</span><span class="n">head_pool_kernel_size</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">head_activation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">head</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Net</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">blocks</span><span class="p">))</span></div>


<div class="viewcode-block" id="ResBlock"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.ResBlock">[docs]</a><span class="k">class</span> <span class="nc">ResBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Residual block. Performs a summation between an identity shortcut in branch1 and a</span>
<span class="sd">    main block in branch2. When the input and output dimensions are different, a</span>
<span class="sd">    convolution followed by a normalization will be performed.</span>

<span class="sd">    ::</span>


<span class="sd">                                         Input</span>
<span class="sd">                                           |-------+</span>
<span class="sd">                                           ↓       |</span>
<span class="sd">                                         Block     |</span>
<span class="sd">                                           ↓       |</span>
<span class="sd">                                       Summation ←-+</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                       Activation</span>

<span class="sd">    The builder can be found in `create_res_block`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ResBlock.__init__"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.ResBlock.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">branch1_conv</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch1_norm</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch2</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">branch_fusion</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            branch1_conv (torch.nn.modules): convolutional module in branch1.</span>
<span class="sd">            branch1_norm (torch.nn.modules): normalization module in branch1.</span>
<span class="sd">            branch2 (torch.nn.modules): bottleneck block module in branch2.</span>
<span class="sd">            activation (torch.nn.modules): activation module.</span>
<span class="sd">            branch_fusion: (Callable): A callable or layer that combines branch1</span>
<span class="sd">                and branch2.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">set_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch1_conv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_fusion</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shortcut</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch1_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch1_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">shortcut</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch1_norm</span><span class="p">(</span><span class="n">shortcut</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch_fusion</span><span class="p">(</span><span class="n">shortcut</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">branch2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="SeparableBottleneckBlock"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.SeparableBottleneckBlock">[docs]</a><span class="k">class</span> <span class="nc">SeparableBottleneckBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Separable Bottleneck block: a sequence of spatiotemporal Convolution, Normalization,</span>
<span class="sd">    and Activations repeated in the following order. Requires a tuple of models to be</span>
<span class="sd">    provided to conv_b, norm_b, act_b to perform Convolution, Normalization, and</span>
<span class="sd">    Activations in parallel Separably.</span>

<span class="sd">    ::</span>


<span class="sd">                                    Conv3d (conv_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                   Activation (act_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Conv3d(s) (conv_b), ...</span>
<span class="sd">                                         ↓ (↓)</span>
<span class="sd">                              Normalization(s) (norm_b), ...</span>
<span class="sd">                                         ↓ (↓)</span>
<span class="sd">                                 Activation(s) (act_b), ...</span>
<span class="sd">                                         ↓ (↓)</span>
<span class="sd">                                  Reduce (sum or cat)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                    Conv3d (conv_c)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_c)</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SeparableBottleneckBlock.__init__"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.SeparableBottleneckBlock.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">conv_a</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">norm_a</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">act_a</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">conv_b</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span>
        <span class="n">norm_b</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span>
        <span class="n">act_b</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">,</span>
        <span class="n">conv_c</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">norm_c</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">reduce_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            conv_a (torch.nn.modules): convolutional module.</span>
<span class="sd">            norm_a (torch.nn.modules): normalization module.</span>
<span class="sd">            act_a (torch.nn.modules): activation module.</span>
<span class="sd">            conv_b (torch.nn.modules_list): convolutional module(s).</span>
<span class="sd">            norm_b (torch.nn.modules_list): normalization module(s).</span>
<span class="sd">            act_b (torch.nn.modules_list): activation module(s).</span>
<span class="sd">            conv_c (torch.nn.modules): convolutional module.</span>
<span class="sd">            norm_c (torch.nn.modules): normalization module.</span>
<span class="sd">            reduce_method (str): if multiple conv_b is used, reduce the output with</span>
<span class="sd">                `sum`, or `cat`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">set_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_c</span><span class="p">)</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_a</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_b</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_c</span><span class="si">}</span><span class="s2"> has None&quot;</span>
        <span class="k">assert</span> <span class="n">reduce_method</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># This flag is used for weight initialization.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span><span class="o">.</span><span class="n">block_final_bn</span> <span class="o">=</span> <span class="kc">True</span></div>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Explicitly forward every layer.</span>
        <span class="c1"># Branch2a, for example Tx1x1, BN, ReLU.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Branch2b, for example 1xHxW, BN, ReLU.</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_b</span><span class="p">)):</span>
            <span class="n">x_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_b</span><span class="p">[</span><span class="n">ind</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_b</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_b</span><span class="p">[</span><span class="n">ind</span><span class="p">](</span><span class="n">x_</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_b</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_b</span><span class="p">[</span><span class="n">ind</span><span class="p">](</span><span class="n">x_</span><span class="p">)</span>
            <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_method</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_method</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Branch2c, for example 1x1x1, BN.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="BottleneckBlock"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.BottleneckBlock">[docs]</a><span class="k">class</span> <span class="nc">BottleneckBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Bottleneck block: a sequence of spatiotemporal Convolution, Normalization,</span>
<span class="sd">    and Activations repeated in the following order:</span>

<span class="sd">    ::</span>


<span class="sd">                                    Conv3d (conv_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                   Activation (act_a)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                    Conv3d (conv_b)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_b)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                   Activation (act_b)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                    Conv3d (conv_c)</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                 Normalization (norm_c)</span>

<span class="sd">    The builder can be found in `create_bottleneck_block`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="BottleneckBlock.__init__"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.BottleneckBlock.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">conv_a</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_a</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">act_a</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_b</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_b</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">act_b</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_c</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_c</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            conv_a (torch.nn.modules): convolutional module.</span>
<span class="sd">            norm_a (torch.nn.modules): normalization module.</span>
<span class="sd">            act_a (torch.nn.modules): activation module.</span>
<span class="sd">            conv_b (torch.nn.modules): convolutional module.</span>
<span class="sd">            norm_b (torch.nn.modules): normalization module.</span>
<span class="sd">            act_b (torch.nn.modules): activation module.</span>
<span class="sd">            conv_c (torch.nn.modules): convolutional module.</span>
<span class="sd">            norm_c (torch.nn.modules): normalization module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">set_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">op</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_c</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># This flag is used for weight initialization.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span><span class="o">.</span><span class="n">block_final_bn</span> <span class="o">=</span> <span class="kc">True</span></div>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Explicitly forward every layer.</span>
        <span class="c1"># Branch2a, for example Tx1x1, BN, ReLU.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Branch2b, for example 1xHxW, BN, ReLU.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_b</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_b</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_b</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Branch2c, for example 1x1x1, BN.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_c</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="ResStage"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.ResStage">[docs]</a><span class="k">class</span> <span class="nc">ResStage</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ResStage composes sequential blocks that make up a ResNet. These blocks could be,</span>
<span class="sd">    for example, Residual blocks, Non-Local layers, or Squeeze-Excitation layers.</span>

<span class="sd">    ::</span>


<span class="sd">                                        Input</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                       ResBlock</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           .</span>
<span class="sd">                                           ↓</span>
<span class="sd">                                       ResBlock</span>

<span class="sd">    The builder can be found in `create_res_stage`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ResStage.__init__"><a class="viewcode-back" href="../../../api/models/resnet.html#pytorchvideo.models.resnet.ResStage.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">res_blocks</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            res_blocks (torch.nn.module_list): ResBlock module(s).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">res_blocks</span> <span class="o">=</span> <span class="n">res_blocks</span></div>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">res_block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">res_blocks</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">res_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>
</pre></div>

             </article>
             
            </div>
            <!-- <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, PyTorchVideo contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
 -->
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorchvideo.org" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorchvideo.org">Home</a>
          </li>
          <li>
            <a href="https://pytorchvideo.org/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>