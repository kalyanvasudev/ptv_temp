


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>id: tutorial_classification title: Training a PyTorchVideo classification model &mdash; PyTorchVideo  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  
  
  
    <link rel="canonical" href="https://pytorchvideo.org/docs/docs/tutorial_classification.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorchvideo.org" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorchvideo.org">Home</a>
          </li>
          <li>
            <a href="https://pytorchvideo.org/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="models.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">Model Zoo and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/models/index.html">Models API</a></li>
</ul>
<p class="caption"><span class="caption-text">Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/data/index.html">Data API</a></li>
</ul>
<p class="caption"><span class="caption-text">Transforms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/transforms/index.html">Transforms API</a></li>
</ul>
<p class="caption"><span class="caption-text">Layers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/layers/index.html">Layers API</a></li>
</ul>
<p class="caption"><span class="caption-text">Accelerator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">Overview</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>id: tutorial_classification
title: Training a PyTorchVideo classification model</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/docs/tutorial_classification.md.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <hr class="docutils" />
<div class="section" id="id-tutorial-classification-title-training-a-pytorchvideo-classification-model">
<h1>id: tutorial_classification
title: Training a PyTorchVideo classification model<a class="headerlink" href="#id-tutorial-classification-title-training-a-pytorchvideo-classification-model" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial we will show how to build a simple video classification training pipeline using PyTorchVideo models, datasets and transforms. We’ll be using a 3D ResNet [1] for the model, Kinetics [2] for the dataset and a standard video transform augmentation recipe. As PyTorchVideo doesn’t contain training code, we’ll use <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning">PyTorch Lightning</a> - a lightweight PyTorch training framework - to help out. Don’t worry if you don’t have Lightning experience, we’ll explain what’s needed as we go along.</p>
<p>[1] He, Kaiming, et al. Deep Residual Learning for Image Recognition. ArXiv:1512.03385, 2015.</p>
<p>[2] W. Kay, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.</p>
</div>
<div class="section" id="dataset">
<h1>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h1>
<p>To start off with, let’s setup the PyTorchVideo Kinetics data loader using a <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule">pytorch_lightning_LightningDataModule</a> . A LightningDataModule is a wrapper that defines the train, val and test data partitions, we’ll use it to wrap the PyTorchVideo Kinetics dataset below.</p>
<p>The PyTorchVideo Kinetics dataset is just an alias for the general <a class="reference external" href="http://pytorchvideo.org/api/data/encoded_video.html#pytorchvideo.data.encoded_video_dataset.EncodedVideoDataset">pytorchvideo.data.EncodedVideoDataset</a> class. If you look at its constructor, you’ll notice that most args are what you’d expect (e.g. path to data). However, there are a few args that are more specific to PyTorchVideo datasets:</p>
<ul class="simple">
<li><p>video_sampler - defining the order to sample a video at each iteration. The default is a “random”.</p></li>
<li><p>clip_sampler - defining how to sample a clip from the chosen video at each iteration. For a train partition it is typical to use a “random” clip sampler (i.e. take a random clip of the specified duration from the video). For testing, typically you’ll use “uniform” (i.e. uniformly sample all clips of the specified duration from the video) to ensure the entire video is sampled in each epoch.</p></li>
<li><p>transform - this provides a way to apply user defined data preprocessing or augmentation before batch collating by the PyTorch data loader. We’ll show an example using this later.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pytorch_lightning</span>
<span class="kn">import</span> <span class="nn">pytorchvideo.data</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>

<span class="k">class</span> <span class="nc">KineticsDataModule</span><span class="p">(</span><span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">LightningDataModule</span><span class="p">):</span>

  <span class="c1"># Dataset configuration</span>
  <span class="n">_DATA_PATH</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">path_to_kinetics_data_dir</span><span class="o">&gt;</span>
  <span class="n">_CLIP_DURATION</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Duration of sampled clip for each video</span>
  <span class="n">_BATCH_SIZE</span> <span class="o">=</span> <span class="mi">8</span>
  <span class="n">_NUM_WORKERS</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># Number of parallel processes fetching data</span>

  <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create the Kinetics train partition from the list of video labels</span>
<span class="sd">    in {self._DATA_PATH}/train.csv</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">pytorchvideo</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Kinetics</span><span class="p">(</span>
        <span class="n">data_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_DATA_PATH</span><span class="p">,</span> <span class="s2">&quot;train.csv&quot;</span><span class="p">),</span>
        <span class="n">clip_sampler</span><span class="o">=</span><span class="n">pytorchvideo</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">make_clip_sampler</span><span class="p">(</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CLIP_DURATION</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">train_dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_BATCH_SIZE</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_NUM_WORKERS</span><span class="p">,</span>
    <span class="p">)</span>

  <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create the Kinetics validation partition from the list of video labels</span>
<span class="sd">    in {self._DATA_PATH}/train.csv</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">pytorchvideo</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Kinetics</span><span class="p">(</span>
        <span class="n">data_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_DATA_PATH</span><span class="p">,</span> <span class="s2">&quot;val.csv&quot;</span><span class="p">),</span>
        <span class="n">clip_sampler</span><span class="o">=</span><span class="n">pytorchvideo</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">make_clip_sampler</span><span class="p">(</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CLIP_DURATION</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">val_dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_BATCH_SIZE</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_NUM_WORKERS</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="transforms">
<h1>Transforms<a class="headerlink" href="#transforms" title="Permalink to this headline">¶</a></h1>
<p>As mentioned above, PyTorchVideo datasets take a “transform” callable arg that defines custom processing (e.g. augmentations, normalization) that’s applied to each clip. The callable arg takes a clip dictionary defining the different modalities and metadata. pytorchvideo.data.Kinetics clips have the following dictionary format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="p">{</span>
     <span class="s1">&#39;video&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">video_tensor</span><span class="o">&gt;</span><span class="p">,</span>     <span class="c1"># Shape: (C, T, H, W)</span>
     <span class="s1">&#39;audio&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">audio_tensor</span><span class="o">&gt;</span><span class="p">,</span>     <span class="c1"># Shape: (S)</span>
     <span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">action_label</span><span class="o">&gt;</span><span class="p">,</span>     <span class="c1"># Integer defining class annotation</span>
     <span class="s1">&#39;video_name&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">video_path</span><span class="o">&gt;</span><span class="p">,</span>  <span class="c1"># Video file path stem</span>
     <span class="s1">&#39;video_index&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">video_id</span><span class="o">&gt;</span><span class="p">,</span>   <span class="c1"># index of video used by sampler</span>
     <span class="s1">&#39;clip_index&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">clip_id</span><span class="o">&gt;</span>      <span class="c1"># index of clip sampled within video</span>
  <span class="p">}</span>
</pre></div>
</div>
<p>PyTorchVideo provides several transforms which you can see in the <a class="reference external" href="https://pytorchvideo.org/docs/transforms.html">docs</a> Notably, PyTorchVideo provides dictionary transforms that can be used to easily interoperate with other domain specific libraries. For example, <a class="reference external" href="https://pytorchvideo.org/docs/_modules/pytorchvideo/transforms/transforms.html#ApplyTransformToKey">pytorchvideo.transforms.ApplyTransformToKey(key, transform)</a>, can be used to apply domain specific transforms to a specific dictionary key. For video tensors we use the same tensor shape as TorchVision and for audio we use TorchAudio tensor shapes, making it east to apply their transforms alongside PyTorchVideo ones.</p>
<p>Below we revise the LightningDataModule from the last section to include transforms coming from both TorchVision and PyTorchVideo. For brevity we’ll just show the KineticsDataModule.train_dataloader method. The validation dataset transforms would be the same just without the augmentations (RandomShortSideScale, RandomCropVideo, RandomHorizontalFlipVideo).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorchvideo.transforms</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ApplyTransformToKey</span><span class="p">,</span>
    <span class="n">RandomShortSideScale</span><span class="p">,</span>
    <span class="n">RemoveKey</span><span class="p">,</span>
    <span class="n">ShortSideScale</span><span class="p">,</span>
    <span class="n">UniformTemporalSubsample</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Compose</span><span class="p">,</span>
    <span class="n">Normalize</span><span class="p">,</span>
    <span class="n">RandomCrop</span><span class="p">,</span>
    <span class="n">RandomHorizontalFlip</span>
<span class="p">)</span>

<span class="k">class</span> <span class="nc">KineticsDataModule</span><span class="p">(</span><span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">LightningDataModule</span><span class="p">):</span>

<span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the Kinetics train partition from the list of video labels</span>
<span class="sd">        in {self._DATA_PATH}/train.csv. Add transform that subsamples and</span>
<span class="sd">        normalizes the video before applying the scale, crop and flip augmentations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span>
            <span class="p">[</span>
            <span class="n">ApplyTransformToKey</span><span class="p">(</span>
              <span class="n">key</span><span class="o">=</span><span class="s2">&quot;video&quot;</span><span class="p">,</span>
              <span class="n">transform</span><span class="o">=</span><span class="n">Compose</span><span class="p">(</span>
                  <span class="p">[</span>
                    <span class="n">UniformTemporalSubsample</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>
                    <span class="n">Normalize</span><span class="p">((</span><span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.225</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">)),</span>
                    <span class="n">RandomShortSideScale</span><span class="p">(</span><span class="n">min_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">320</span><span class="p">),</span>
                    <span class="n">RandomCrop</span><span class="p">(</span><span class="mi">244</span><span class="p">),</span>
                    <span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
                  <span class="p">]</span>
                <span class="p">),</span>
              <span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">pytorchvideo</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Kinetics</span><span class="p">(</span>
            <span class="n">data_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_DATA_PATH</span><span class="p">,</span> <span class="s2">&quot;train.csv&quot;</span><span class="p">),</span>
            <span class="n">clip_sampler</span><span class="o">=</span><span class="n">pytorchvideo</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">make_clip_sampler</span><span class="p">(</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_CLIP_DURATION</span><span class="p">),</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">train_transform</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">train_dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_BATCH_SIZE</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_NUM_WORKERS</span><span class="p">,</span>
        <span class="p">)</span>

<span class="c1"># ...</span>
</pre></div>
</div>
</div>
<div class="section" id="model">
<h1>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h1>
<p>All PyTorchVideo models and layers can be built with simple, reproducible factory functions. We call this the “flat” model interface because the args don’t require hierachies of configs to be used. An example building a default ResNet can be found below. See the <a class="reference external" href="https://pytorchvideo.readthedocs.io/en/latest/_modules/pytorchvideo/models/resnet.html#create_bottleneck_block">docs</a> for more configuration options.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytorchvideo.models.resnet</span>

<span class="k">def</span> <span class="nf">make_kinetics_resnet</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">pytorchvideo</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet</span><span class="o">.</span><span class="n">create_resnet</span><span class="p">(</span>
      <span class="n">input_channel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># RGB input from Kinetics</span>
      <span class="n">model_depth</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="c1"># For the tutorial let&#39;s just use a 50 layer network</span>
      <span class="n">model_num_class</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="c1"># Kinetics has 400 classes so we need out final head to align</span>
      <span class="n">norm</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
      <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
  <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="putting-it-all-together">
<h1>Putting it all together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this headline">¶</a></h1>
<p>To put everything together, let’s create a <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html">pytorch_lightning.LightningModule</a>. This defines the train and validation step code (i.e. the code inside the training and evaluation loops), and the optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">VideoClassificationLightningModule</span><span class="p">(</span><span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">make_kinetics_resnet</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
      <span class="c1"># The model expects a video tensor of shape (B, C, T, H, W), which is the </span>
      <span class="c1"># format provided by the dataset</span>
      <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;video&quot;</span><span class="p">])</span>

      <span class="c1"># Compute cross entropy loss, loss.backwards will be called behind the scenes</span>
      <span class="c1"># by PyTorchLightning after being returned from this method.</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">])</span>

      <span class="c1"># Log the train loss to Tensorboard</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

      <span class="k">return</span> <span class="n">loss</span>

  <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
      <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;video&quot;</span><span class="p">])</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">loss</span>

  <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is</span>
<span class="sd">      usually useful for training video models.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>
</pre></div>
</div>
<p>Our VideoClassificationLightningModule and KineticsDataModule are ready be trained together using the <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html">pytorch_lightning.Trainer</a>!. The trainer class has many arguments to define the training environment (e.g. num_gpus, distributed_backend). To keep things simple we’ll just use the default local cpu training but note that this would likely take weeks to train so you might want to use more performant settings based on your environment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="n">classification_module</span> <span class="o">=</span> <span class="n">VideoClassificationLightningModule</span><span class="p">()</span>
    <span class="n">data_module</span> <span class="o">=</span> <span class="n">KineticsDataModule</span><span class="p">()</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pytorch_lightning</span><span class="o">.</span><span class="n">Trainer</span><span class="p">()</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">classification_module</span><span class="p">,</span> <span class="n">data_module</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial we showed how to train a 3D ResNet on Kinetics using PyTorch Lightning. You can see the final code from the tutorial (including a few extra bells and whistles) in the PyTorchVideo projects directory.</p>
<p>To learn more about PyTorchVideo, check out the rest of the <a class="reference external" href="https://pytorchvideo.org/docs/">documentation</a>  and <a class="reference external" href="https://pytorchvideo.org/docs/tutorial_overview">tutorials</a>.</p>
</div>


             </article>
             
            </div>
            <!-- <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, PyTorchVideo contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
 -->
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">id: tutorial_classification
title: Training a PyTorchVideo classification model</a></li>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#dataset">Dataset</a></li>
<li><a class="reference internal" href="#transforms">Transforms</a></li>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#putting-it-all-together">Putting it all together</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorchvideo.org" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorchvideo.org">Home</a>
          </li>
          <li>
            <a href="https://pytorchvideo.org/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>