


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>id: tutorial_accelerator_build_your_model title: “Build your efficient model with PytorchVideo/Accelerator” &mdash; PyTorchVideo  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  
  
  
    <link rel="canonical" href="https://pytorchvideo.org/docs/docs/tutorial_accelerator_build_your_model.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorchvideo.org" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorchvideo.org">Home</a>
          </li>
          <li>
            <a href="https://pytorchvideo.org/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="models.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">Model Zoo and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/models/index.html">Models API</a></li>
</ul>
<p class="caption"><span class="caption-text">Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/data/index.html">Data API</a></li>
</ul>
<p class="caption"><span class="caption-text">Transforms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/transforms/index.html">Transforms API</a></li>
</ul>
<p class="caption"><span class="caption-text">Layers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/layers/index.html">Layers API</a></li>
</ul>
<p class="caption"><span class="caption-text">Accelerator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">Overview</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>id: tutorial_accelerator_build_your_model
title: “Build your efficient model with PytorchVideo/Accelerator”</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/docs/tutorial_accelerator_build_your_model.md.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <hr class="docutils" />
<div class="section" id="id-tutorial-accelerator-build-your-model-title-build-your-efficient-model-with-pytorchvideo-accelerator">
<h1>id: tutorial_accelerator_build_your_model
title: “Build your efficient model with PytorchVideo/Accelerator”<a class="headerlink" href="#id-tutorial-accelerator-build-your-model-title-build-your-efficient-model-with-pytorchvideo-accelerator" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we will go through:</p>
<ul class="simple">
<li><p>Basics of efficient blocks in PytorchVideo/Accelerator;</p></li>
<li><p>Design, train and deploy a model composed of efficient blocks for mobile CPU.</p></li>
</ul>
</div>
<div class="section" id="basics-of-efficient-blocks-in-pytorchvideo-accelerator">
<h1>Basics of efficient blocks in PytorchVideo/Accelerator<a class="headerlink" href="#basics-of-efficient-blocks-in-pytorchvideo-accelerator" title="Permalink to this headline">¶</a></h1>
<p>Efficient blocks are blocks with high efficiency. For a target device, we benchmark efficiency of basic network components and provide a collection of efficient blocks under <code class="docutils literal notranslate"><span class="pre">pytorchvideo/layers/accelerator/&lt;target_device&gt;</span></code> (for simple layers) and <code class="docutils literal notranslate"><span class="pre">pytorchvideo/models/accelerator/&lt;target_device&gt;</span></code> (for complex modules such as residual block). Inferencing of a model built up with corresponding efficient blocks on target device is guranteed to be efficient.</p>
<p>Each efficient block module is an instance of nn.Module, and has two forms: <strong>original form</strong> (for training) and <strong>deploy form</strong> (for inference). When in original form, the efficient block module has exactly the same behavior as a corresponding vanilla nn.Module for both forward and backward operation. User can freely mix and match efficient blocks for the same target device and build up their own model. Once model is built and trained, user can convert each efficient block in model into deploy form. The conversion will do graph and kernel optimization on each efficient block, and efficient block in deploy form is arithmetically equivalent to original form but has much higher efficiency during inference.</p>
</div>
<div class="section" id="design-train-and-deploy-a-model-composed-of-efficient-blocks-for-mobile-cpu">
<h1>Design, train and deploy a model composed of efficient blocks for mobile CPU<a class="headerlink" href="#design-train-and-deploy-a-model-composed-of-efficient-blocks-for-mobile-cpu" title="Permalink to this headline">¶</a></h1>
<div class="section" id="build-a-model">
<h2>Build a model<a class="headerlink" href="#build-a-model" title="Permalink to this headline">¶</a></h2>
<p>In this section, let’s go through the process of design, train and deploy using a example toy model using efficient blocks under <code class="docutils literal notranslate"><span class="pre">pytorchvideo/layers/accelerator/mobile_cpu</span></code> and <code class="docutils literal notranslate"><span class="pre">pytorchvideo/models/accelerator/mobile_cpu</span></code>, which includes:</p>
<ul class="simple">
<li><p>One conv3d head layer with 5x1x1 kernel followed by ReLU activation;</p></li>
<li><p>One residual block with squeeze-excite;</p></li>
<li><p>One average pool and fully connected layer as final output.</p></li>
</ul>
<p>First, let’s import efficient blocks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.layers.accelerator.mobile_cpu.activation_functions</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">supported_act_functions</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.layers.accelerator.mobile_cpu.convolutions</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Conv3d5x1x1BnAct</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.models.accelerator.mobile_cpu.residual_blocks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">X3dBottleneckBlock</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.layers.accelerator.mobile_cpu.pool</span> <span class="kn">import</span> <span class="n">AdaptiveAvgPool3dOutSize1</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.layers.accelerator.mobile_cpu.fully_connected</span> <span class="kn">import</span> <span class="n">FullyConnected</span>
</pre></div>
</div>
<p>Then we can build a model using those efficient blocks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># input channel of first 5x1x1 layer</span>
        <span class="n">residual_block_channel</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>  <span class="c1"># input channel of residual block</span>
        <span class="n">expansion_ratio</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># expansion ratio of residual block</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="c1"># final output classes</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># s1 - 5x1x1 conv3d layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s1</span> <span class="o">=</span> <span class="n">Conv3d5x1x1BnAct</span><span class="p">(</span>
            <span class="n">in_channel</span><span class="p">,</span>
            <span class="n">residual_block_channel</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">use_bn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># s2 - residual block</span>
        <span class="n">mid_channel</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">residual_block_channel</span> <span class="o">*</span> <span class="n">expansion_ratio</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s2</span> <span class="o">=</span> <span class="n">X3dBottleneckBlock</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">residual_block_channel</span><span class="p">,</span>
                <span class="n">mid_channels</span><span class="o">=</span><span class="n">mid_channel</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">residual_block_channel</span><span class="p">,</span>
                <span class="n">use_residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">spatial_stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">se_ratio</span><span class="o">=</span><span class="mf">0.0625</span><span class="p">,</span>
                <span class="n">act_functions</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;swish&quot;</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">),</span>
                <span class="n">use_bn</span><span class="o">=</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="c1"># Average pool and fully connected layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">AdaptiveAvgPool3dOutSize1</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">FullyConnected</span><span class="p">(</span><span class="n">residual_block_channel</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">supported_act_functions</span><span class="p">[</span><span class="s1">&#39;relu&#39;</span><span class="p">]()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># (N, C, T, H, W) -&gt; (N, T, H, W, C).</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Performs fully convolutional inference.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>We can instantiate MyNet and its efficient blocks will be in original form.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net_inst</span> <span class="o">=</span> <span class="n">MyNet</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net_inst</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MyNet</span><span class="p">(</span>
  <span class="p">(</span><span class="n">s1</span><span class="p">):</span> <span class="n">Conv3d5x1x1BnAct</span><span class="p">(</span>
    <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">Conv3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
        <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">s2</span><span class="p">):</span> <span class="n">X3dBottleneckBlock</span><span class="p">(</span>
    <span class="p">(</span><span class="n">_residual_add_func</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
      <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">final_act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
      <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv_0</span><span class="p">):</span> <span class="n">Conv3dPwBnAct</span><span class="p">(</span>
        <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">Conv3d</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
          <span class="p">(</span><span class="n">bn</span><span class="p">):</span> <span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
            <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">conv_1</span><span class="p">):</span> <span class="n">Conv3d3x3x3DwBnAct</span><span class="p">(</span>
        <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">Conv3d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
          <span class="p">(</span><span class="n">bn</span><span class="p">):</span> <span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">(</span>
            <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">se</span><span class="p">):</span> <span class="n">SqueezeExcitation</span><span class="p">(</span>
        <span class="p">(</span><span class="n">se</span><span class="p">):</span> <span class="n">SqueezeExcitation</span><span class="p">(</span>
          <span class="p">(</span><span class="n">block</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv3d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Conv3d</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Sigmoid</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">act_func_1</span><span class="p">):</span> <span class="n">Swish</span><span class="p">(</span>
        <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Swish</span><span class="p">()</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">conv_2</span><span class="p">):</span> <span class="n">Conv3dPwBnAct</span><span class="p">(</span>
        <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">Conv3d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
          <span class="p">(</span><span class="n">bn</span><span class="p">):</span> <span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">(</span>
            <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">avg_pool</span><span class="p">):</span> <span class="n">AdaptiveAvgPool3dOutSize1</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pool</span><span class="p">):</span> <span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">projection</span><span class="p">):</span> <span class="n">FullyConnected</span><span class="p">(</span>
    <span class="p">(</span><span class="n">model</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
    <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="train-model">
<h2>Train model<a class="headerlink" href="#train-model" title="Permalink to this headline">¶</a></h2>
<p>Then we can train the model with your dataset/optimizer. Here we skip this training step, and just leave the weight as initial value.</p>
</div>
<div class="section" id="deploy-model">
<h2>Deploy model<a class="headerlink" href="#deploy-model" title="Permalink to this headline">¶</a></h2>
<p>Now the model is ready to deploy. First of all, let’s convert the model into deploy form. In order to do that, we need to use <code class="docutils literal notranslate"><span class="pre">convert_to_deployable_form</span></code> utility and provide an example input tensor to the model. Note that once the model is converted into deploy form, the input size should be the same as the example input tensor size during conversion.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.accelerator.deployment.mobile_cpu.utils.model_conversion</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_to_deployable_form</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">input_blob_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_blob_size</span><span class="p">)</span>
<span class="n">net_inst_deploy</span> <span class="o">=</span> <span class="n">convert_to_deployable_form</span><span class="p">(</span><span class="n">net_inst</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
<p>We can see that the network graph has been changed after conversion, which did kernel and graph optimization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">net_inst_deploy</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MyNet</span><span class="p">(</span>
  <span class="p">(</span><span class="n">s1</span><span class="p">):</span> <span class="n">Conv3d5x1x1BnAct</span><span class="p">(</span>
    <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">_Conv3dTemporalKernel5Decomposed</span><span class="p">(</span>
        <span class="p">(</span><span class="n">_conv2d_0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="n">_conv2d_1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="n">_conv2d_2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="n">_conv2d_3</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="n">_conv2d_4</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="n">_add_funcs</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">7</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">8</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">9</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">_cat_func</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
          <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
        <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">s2</span><span class="p">):</span> <span class="n">X3dBottleneckBlock</span><span class="p">(</span>
    <span class="p">(</span><span class="n">_residual_add_func</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
      <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">final_act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
      <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv_0</span><span class="p">):</span> <span class="n">Conv3dPwBnAct</span><span class="p">(</span>
        <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">_Reshape</span><span class="p">()</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">ConvReLU2d</span><span class="p">(</span>
              <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
              <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">bn</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
            <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
              <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
            <span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">_Reshape</span><span class="p">()</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">conv_1</span><span class="p">):</span> <span class="n">Conv3d3x3x3DwBnAct</span><span class="p">(</span>
        <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">_Conv3dTemporalKernel3Decomposed</span><span class="p">(</span>
            <span class="p">(</span><span class="n">_conv2d_3_3_0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">(</span><span class="n">_conv2d_3_3_2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">(</span><span class="n">_conv2d_3_3_1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="mi">72</span><span class="p">)</span>
            <span class="p">(</span><span class="n">_add_funcs</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
              <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
                <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
              <span class="p">)</span>
              <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
                <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
              <span class="p">)</span>
              <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
                <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
              <span class="p">)</span>
              <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
                <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
              <span class="p">)</span>
              <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
                <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
              <span class="p">)</span>
              <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
                <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
              <span class="p">)</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">_cat_func</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
              <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
            <span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">bn</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">(</span>
            <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">se</span><span class="p">):</span> <span class="n">SqueezeExcitation</span><span class="p">(</span>
        <span class="p">(</span><span class="n">se</span><span class="p">):</span> <span class="n">_SkipConnectMul</span><span class="p">(</span>
          <span class="p">(</span><span class="n">layer</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">_Reshape</span><span class="p">()</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">()</span>
            <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="n">Sigmoid</span><span class="p">()</span>
            <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">_Reshape</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">mul_func</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">act_func_1</span><span class="p">):</span> <span class="n">Swish</span><span class="p">(</span>
        <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">_NaiveSwish</span><span class="p">(</span>
          <span class="p">(</span><span class="n">mul_func</span><span class="p">):</span> <span class="n">FloatFunctional</span><span class="p">(</span>
            <span class="p">(</span><span class="n">activation_post_process</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">conv_2</span><span class="p">):</span> <span class="n">Conv3dPwBnAct</span><span class="p">(</span>
        <span class="p">(</span><span class="n">kernel</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">_Reshape</span><span class="p">()</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="p">(</span><span class="n">conv</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="p">(</span><span class="n">bn</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
            <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">(</span>
              <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
            <span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">_Reshape</span><span class="p">()</span>
        <span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">avg_pool</span><span class="p">):</span> <span class="n">AdaptiveAvgPool3dOutSize1</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pool</span><span class="p">):</span> <span class="n">AvgPool3d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">projection</span><span class="p">):</span> <span class="n">FullyConnected</span><span class="p">(</span>
    <span class="p">(</span><span class="n">model</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span>
    <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let’s check whether the network after conversion is arithmetically equivalent. We expect the output to be very close before/after conversion, with some small difference due to numeric noise from floating point operation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net_inst</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">out_ref</span> <span class="o">=</span> <span class="n">net_inst</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net_inst_deploy</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>

<span class="n">max_err</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out_ref</span> <span class="o">-</span> <span class="n">out</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;max error is </span><span class="si">{</span><span class="n">max_err</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">max</span> <span class="n">error</span> <span class="ow">is</span> <span class="mf">2.9802322387695312e-08</span>
</pre></div>
</div>
<p>Next we have two options: either deploy floating point model, or quantize model into int8 and then deploy.</p>
<p>Let’s first assume we want to deploy floating point model. In this case, all we need to do is to export jit trace and then apply <code class="docutils literal notranslate"><span class="pre">optimize_for_mobile</span></code> for final optimization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.mobile_optimizer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">optimize_for_mobile</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">net_inst_deploy</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">traced_model_opt</span> <span class="o">=</span> <span class="n">optimize_for_mobile</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># Here we can save the traced_model_opt to JIT file using traced_model_opt.save(&lt;file_path&gt;)</span>
</pre></div>
</div>
<p>Alternatively, we may also want to deploy a quantized model. Efficient blocks are quantization-friendly by design - just wrap the model in deploy form with <code class="docutils literal notranslate"><span class="pre">QuantStub/DeQuantStub</span></code> and it is ready for Pytorch eager mode quantization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wrapper class for adding QuantStub/DeQuantStub.</span>
<span class="k">class</span> <span class="nc">quant_stub_wrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_in</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantStub</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">module_in</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">DeQuantStub</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net_inst_quant_stub_wrapper</span> <span class="o">=</span> <span class="n">quant_stub_wrapper</span><span class="p">(</span><span class="n">net_inst_deploy</span><span class="p">)</span>
</pre></div>
</div>
<p>Preparation step of quantization. Fusion has been done for efficient blocks automatically during <code class="docutils literal notranslate"><span class="pre">convert_to_deployable_form</span></code>, so we can just proceed to <code class="docutils literal notranslate"><span class="pre">torch.quantization.prepare</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net_inst_quant_stub_wrapper</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_qconfig</span>
<span class="n">net_inst_quant_stub_wrapper_prepared</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">net_inst_quant_stub_wrapper</span><span class="p">)</span>
</pre></div>
</div>
<p>Calibration and quantization. After preparation we will do calibration of quantization by feeding calibration dataset (skipped here) and then do quantization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># calibration is skipped here.</span>
<span class="n">net_inst_quant_stub_wrapper_quantized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">net_inst_quant_stub_wrapper_prepared</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we can export trace of int8 model and deploy on mobile devices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">traced_model_int8</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">net_inst_quant_stub_wrapper_quantized</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">traced_model_int8_opt</span> <span class="o">=</span> <span class="n">optimize_for_mobile</span><span class="p">(</span><span class="n">traced_model_int8</span><span class="p">)</span>
<span class="c1"># Here we can save the traced_model_opt to JIT file using traced_model_int8_opt.save(&lt;file_path&gt;)</span>
</pre></div>
</div>
</div>
</div>


             </article>
             
            </div>
            <!-- <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, PyTorchVideo contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
 -->
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">id: tutorial_accelerator_build_your_model
title: “Build your efficient model with PytorchVideo/Accelerator”</a></li>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#basics-of-efficient-blocks-in-pytorchvideo-accelerator">Basics of efficient blocks in PytorchVideo/Accelerator</a></li>
<li><a class="reference internal" href="#design-train-and-deploy-a-model-composed-of-efficient-blocks-for-mobile-cpu">Design, train and deploy a model composed of efficient blocks for mobile CPU</a><ul>
<li><a class="reference internal" href="#build-a-model">Build a model</a></li>
<li><a class="reference internal" href="#train-model">Train model</a></li>
<li><a class="reference internal" href="#deploy-model">Deploy model</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorchvideo.org" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorchvideo.org">Home</a>
          </li>
          <li>
            <a href="https://pytorchvideo.org/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>