


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pytorchvideo.data.domsev &mdash; PyTorchVideo  documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://kalyanvasudev.github.io/api/_modules/pytorchvideo/data/domsev.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://kalyanvasudev.github.io/ptv_temp/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://kalyanvasudev.github.io/ptv_temp/">Home</a>
          </li>
          <li>
            <a href="https://kalyanvasudev.github.io/ptv_temp/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html">PyTorchVideo/Model</a></li>
</ul>
<p class="caption"><span class="caption-text">Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Transforms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../transforms.html">Overview</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Zoo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">PyTorchVideo Model Zoo and Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data_preparation.html">Data Preparation</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index.html">API Documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>pytorchvideo.data.domsev</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for pytorchvideo.data.domsev</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">fields</span> <span class="k">as</span> <span class="n">dataclass_fields</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.data.dataset_manifest_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">EncodedVideoInfo</span><span class="p">,</span>
    <span class="n">VideoClipInfo</span><span class="p">,</span>
    <span class="n">VideoDataset</span><span class="p">,</span>
    <span class="n">VideoDatasetType</span><span class="p">,</span>
    <span class="n">VideoInfo</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.data.utils</span> <span class="kn">import</span> <span class="n">DataclassFieldCaster</span><span class="p">,</span> <span class="n">load_dataclass_dict_from_csv</span>
<span class="kn">from</span> <span class="nn">pytorchvideo.data.video</span> <span class="kn">import</span> <span class="n">Video</span>


<span class="n">USER_SCENE_MAP</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;indoor&quot;</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;nature&quot;</span><span class="p">,</span>
    <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;crowded_environment&quot;</span><span class="p">,</span>
    <span class="mi">4</span><span class="p">:</span> <span class="s2">&quot;urban&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">USER_ACTIVITY_MAP</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;walking&quot;</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;running&quot;</span><span class="p">,</span>
    <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;standing&quot;</span><span class="p">,</span>
    <span class="mi">4</span><span class="p">:</span> <span class="s2">&quot;biking&quot;</span><span class="p">,</span>
    <span class="mi">5</span><span class="p">:</span> <span class="s2">&quot;driving&quot;</span><span class="p">,</span>
    <span class="mi">6</span><span class="p">:</span> <span class="s2">&quot;playing&quot;</span><span class="p">,</span>
    <span class="mi">7</span><span class="p">:</span> <span class="s2">&quot;cooking&quot;</span><span class="p">,</span>
    <span class="mi">8</span><span class="p">:</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span>
    <span class="mi">9</span><span class="p">:</span> <span class="s2">&quot;observing&quot;</span><span class="p">,</span>
    <span class="mi">10</span><span class="p">:</span> <span class="s2">&quot;in_conversation&quot;</span><span class="p">,</span>
    <span class="mi">11</span><span class="p">:</span> <span class="s2">&quot;browsing&quot;</span><span class="p">,</span>
    <span class="mi">12</span><span class="p">:</span> <span class="s2">&quot;shopping&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">USER_ATTENTION_MAP</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;paying_attention&quot;</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;interacting&quot;</span><span class="p">,</span>
<span class="p">}</span>


<div class="viewcode-block" id="ActivityData"><a class="viewcode-back" href="../../../api/data/domsev.html#pytorchvideo.data.domsev.ActivityData">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ActivityData</span><span class="p">(</span><span class="n">DataclassFieldCaster</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class representing a contiguous activity video segment from the DoMSEV dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">video_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">start_time</span><span class="p">:</span> <span class="nb">float</span>  <span class="c1"># Start time of the activity, in seconds</span>
    <span class="n">stop_time</span><span class="p">:</span> <span class="nb">float</span>  <span class="c1"># Stop time of the activity, in seconds</span>
    <span class="n">start_frame</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># 0-indexed ID of the start frame (inclusive)</span>
    <span class="n">stop_frame</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># 0-index ID of the stop frame (inclusive)</span>
    <span class="n">activity_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">activity_name</span><span class="p">:</span> <span class="nb">str</span></div>


<span class="c1"># Utility functions</span>
<div class="viewcode-block" id="seconds_to_frame_index"><a class="viewcode-back" href="../../../api/data/domsev.html#pytorchvideo.data.domsev.seconds_to_frame_index">[docs]</a><span class="k">def</span> <span class="nf">seconds_to_frame_index</span><span class="p">(</span>
    <span class="n">time_in_seconds</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">fps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">zero_indexed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Converts a point in time (in seconds) within a video clip to its closest</span>
<span class="sd">    frame indexed (rounding down), based on a specified frame rate.</span>

<span class="sd">    Args:</span>
<span class="sd">        time_in_seconds (float): The point in time within the video.</span>
<span class="sd">        fps (int): The frame rate (frames per second) of the video.</span>
<span class="sd">        zero_indexed (Optional[bool]): Whether the returned frame should be</span>
<span class="sd">            zero-indexed (if True) or one-indexed (if False).</span>

<span class="sd">    Returns:</span>
<span class="sd">        (int) The index of the nearest frame (rounding down to the nearest integer).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">frame_idx</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">time_in_seconds</span> <span class="o">*</span> <span class="n">fps</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">zero_indexed</span><span class="p">:</span>
        <span class="n">frame_idx</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">frame_idx</span></div>


<div class="viewcode-block" id="frame_index_to_seconds"><a class="viewcode-back" href="../../../api/data/domsev.html#pytorchvideo.data.domsev.frame_index_to_seconds">[docs]</a><span class="k">def</span> <span class="nf">frame_index_to_seconds</span><span class="p">(</span>
    <span class="n">frame_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">fps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">zero_indexed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Converts a frame index within a video clip to the corresponding</span>
<span class="sd">    point in time (in seconds) within the video, based on a specified frame rate.</span>

<span class="sd">    Args:</span>
<span class="sd">        frame_index (int): The index of the frame within the video.</span>
<span class="sd">        fps (int): The frame rate (frames per second) of the video.</span>
<span class="sd">        zero_indexed (Optional[bool]): Whether the specified frame is zero-indexed</span>
<span class="sd">            (if True) or one-indexed (if False).</span>

<span class="sd">    Returns:</span>
<span class="sd">        (float) The point in time within the video.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">zero_indexed</span><span class="p">:</span>
        <span class="n">frame_index</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="n">time_in_seconds</span> <span class="o">=</span> <span class="n">frame_index</span> <span class="o">/</span> <span class="n">fps</span>
    <span class="k">return</span> <span class="n">time_in_seconds</span></div>


<div class="viewcode-block" id="get_overlap_for_time_range_pair"><a class="viewcode-back" href="../../../api/data/domsev.html#pytorchvideo.data.domsev.get_overlap_for_time_range_pair">[docs]</a><span class="k">def</span> <span class="nf">get_overlap_for_time_range_pair</span><span class="p">(</span>
    <span class="n">t1_start</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">t1_stop</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">t2_start</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">t2_stop</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Calculates the overlap between two time ranges, if one exists.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Optional[Tuple]) A tuple of &lt;overlap_start_time, overlap_stop_time&gt; if</span>
<span class="sd">        an overlap is found, or None otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check if there is an overlap</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">t1_start</span> <span class="o">&lt;=</span> <span class="n">t2_stop</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">t2_start</span> <span class="o">&lt;=</span> <span class="n">t1_stop</span><span class="p">):</span>
        <span class="c1"># Calculate the overlap period</span>
        <span class="n">overlap_start_time</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">t1_start</span><span class="p">,</span> <span class="n">t2_start</span><span class="p">)</span>
        <span class="n">overlap_stop_time</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">t1_stop</span><span class="p">,</span> <span class="n">t2_stop</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">overlap_start_time</span><span class="p">,</span> <span class="n">overlap_stop_time</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="DomsevDataset"><a class="viewcode-back" href="../../../api/data/domsev.html#pytorchvideo.data.domsev.DomsevDataset">[docs]</a><span class="k">class</span> <span class="nc">DomsevDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Egocentric activity classification video dataset for DoMSEV stored as</span>
<span class="sd">    an encoded video (with frame-level labels).</span>
<span class="sd">    &lt;https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018-dataset/&gt;</span>

<span class="sd">    This dataset handles the loading, decoding, and configurable clip</span>
<span class="sd">    sampling for the videos.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">video_data_manifest_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">video_info_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">activities_file_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">clip_sampler</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
            <span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Video</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">ActivityData</span><span class="p">]]],</span> <span class="n">List</span><span class="p">[</span><span class="n">VideoClipInfo</span><span class="p">]</span>
        <span class="p">],</span>
        <span class="n">dataset_type</span><span class="p">:</span> <span class="n">VideoDatasetType</span> <span class="o">=</span> <span class="n">VideoDatasetType</span><span class="o">.</span><span class="n">Frame</span><span class="p">,</span>
        <span class="n">frames_per_second</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frame_filter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">multithreaded_io</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        Args:</span>
<span class="s2">            video_data_manifest_file_path (str):</span>
<span class="s2">                The path to a json file outlining the available video data for the</span>
<span class="s2">                associated videos.  File must be a csv (w/header) with columns:</span>
<span class="s2">                </span><span class="si">{</span><span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">dataclass_fields</span><span class="p">(</span><span class="n">EncodedVideoInfo</span><span class="p">)]</span><span class="si">}</span><span class="s2"></span>

<span class="s2">                To generate this file from a directory of video frames, see helper</span>
<span class="s2">                functions in Module: pytorchvideo.data.domsev.utils</span>

<span class="s2">            video_info_file_path (str):</span>
<span class="s2">                Path or URI to manifest with basic metadata of each video.</span>
<span class="s2">                File must be a csv (w/header) with columns:</span>
<span class="s2">                </span><span class="si">{</span><span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">dataclass_fields</span><span class="p">(</span><span class="n">VideoInfo</span><span class="p">)]</span><span class="si">}</span><span class="s2"></span>

<span class="s2">            activities_file_path (str):</span>
<span class="s2">                Path or URI to manifest with activity annotations for each video.</span>
<span class="s2">                File must be a csv (w/header) with columns:</span>
<span class="s2">                </span><span class="si">{</span><span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">dataclass_fields</span><span class="p">(</span><span class="n">ActivityData</span><span class="p">)]</span><span class="si">}</span><span class="s2"></span>

<span class="s2">            clip_sampler: Callable[</span>
<span class="s2">                [Dict[str, Video], Dict[str, List[ActivityData]]], List[VideoClipInfo]</span>
<span class="s2">            ],</span>

<span class="s2">            dataset_type (VideoDatasetType): The dataformat in which dataset</span>
<span class="s2">                video data is store (e.g. video frames, encoded video etc).</span>

<span class="s2">            frames_per_second (int): The FPS of the stored videos. (NOTE:</span>
<span class="s2">                this is variable and may be different than the original FPS</span>
<span class="s2">                reported on the DoMSEV dataset website -- it depends on the</span>
<span class="s2">                subsampling and frame extraction done internally at Facebook).</span>

<span class="s2">            transform (Optional[Callable[[Dict[str, Any]], Any]]):</span>
<span class="s2">                This callable is evaluated on the clip output before the clip is returned.</span>
<span class="s2">                It can be used for user-defined preprocessing and augmentations to the clips.</span>

<span class="s2">                    The clip input is a dictionary with the following format:</span>
<span class="s2">                        </span><span class="se">{{</span><span class="s2"></span>
<span class="s2">                            &#39;video&#39;: &lt;video_tensor&gt;,</span>
<span class="s2">                            &#39;audio&#39;: &lt;audio_tensor&gt;,</span>
<span class="s2">                            &#39;activities&#39;: &lt;activities_tensor&gt;,</span>
<span class="s2">                            &#39;start_time&#39;: &lt;float&gt;,</span>
<span class="s2">                            &#39;stop_time&#39;: &lt;float&gt;</span>
<span class="s2">                        </span><span class="se">}}</span><span class="s2"></span>

<span class="s2">                If transform is None, the raw clip output in the above format is</span>
<span class="s2">                returned unmodified.</span>

<span class="s2">            frame_filter (Optional[Callable[[List[int]], List[int]]]):</span>
<span class="s2">                This callable is evaluated on the set of available frame inidices to be</span>
<span class="s2">                included in a sampled clip. This can be used to subselect frames within</span>
<span class="s2">                a clip to be loaded.</span>

<span class="s2">            multithreaded_io (bool):</span>
<span class="s2">                Boolean to control whether parllelizable io operations are performed across</span>
<span class="s2">                multiple threads.</span>
<span class="s2">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">video_info_file_path</span>
        <span class="k">assert</span> <span class="n">activities_file_path</span>
        <span class="k">assert</span> <span class="n">video_data_manifest_file_path</span>

        <span class="c1"># Populate video and metadata data providers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_videos</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Video</span><span class="p">]</span> <span class="o">=</span> <span class="n">VideoDataset</span><span class="o">.</span><span class="n">_load_videos</span><span class="p">(</span>
            <span class="n">video_data_manifest_file_path</span><span class="p">,</span>
            <span class="n">video_info_file_path</span><span class="p">,</span>
            <span class="n">multithreaded_io</span><span class="p">,</span>
            <span class="n">dataset_type</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_activities</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">ActivityData</span><span class="p">]]</span> <span class="o">=</span> <span class="n">load_dataclass_dict_from_csv</span><span class="p">(</span>
            <span class="n">activities_file_path</span><span class="p">,</span> <span class="n">ActivityData</span><span class="p">,</span> <span class="s2">&quot;video_id&quot;</span><span class="p">,</span> <span class="n">list_per_key</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># Sample datapoints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_clips</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">VideoClipInfo</span><span class="p">]</span> <span class="o">=</span> <span class="n">clip_sampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_videos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activities</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_frames_per_second</span> <span class="o">=</span> <span class="n">frames_per_second</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_user_transform</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_frame_filter</span> <span class="o">=</span> <span class="n">frame_filter</span>

<div class="viewcode-block" id="DomsevDataset.__getitem__"><a class="viewcode-back" href="../../../api/data/domsev.html#pytorchvideo.data.domsev.DomsevDataset.__getitem__">[docs]</a>    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Samples a video clip associated to the given index.</span>

<span class="sd">        Args:</span>
<span class="sd">            index (int): index for the video clip.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A video clip with the following format if transform is None:</span>
<span class="sd">                {{</span>
<span class="sd">                    &#39;video_id&#39;: &lt;str&gt;,</span>
<span class="sd">                    &#39;video&#39;: &lt;video_tensor&gt;,</span>
<span class="sd">                    &#39;audio&#39;: &lt;audio_tensor&gt;,</span>
<span class="sd">                    &#39;activities&#39;: &lt;activities_tensor&gt;,</span>
<span class="sd">                    &#39;start_time&#39;: &lt;float&gt;,</span>
<span class="sd">                    &#39;stop_time&#39;: &lt;float&gt;</span>
<span class="sd">                }}</span>
<span class="sd">            Otherwise, the transform defines the clip output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">clip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clips</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

        <span class="c1"># Filter activities by only the ones that appear within the clip boundaries,</span>
        <span class="c1"># and unpack the activities so there is one per frame in the clip</span>
        <span class="n">activities_in_video</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activities</span><span class="p">[</span><span class="n">clip</span><span class="o">.</span><span class="n">video_id</span><span class="p">]</span>
        <span class="n">activities_in_clip</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">activity</span> <span class="ow">in</span> <span class="n">activities_in_video</span><span class="p">:</span>
            <span class="n">overlap_period</span> <span class="o">=</span> <span class="n">get_overlap_for_time_range_pair</span><span class="p">(</span>
                <span class="n">clip</span><span class="o">.</span><span class="n">start_time</span><span class="p">,</span> <span class="n">clip</span><span class="o">.</span><span class="n">stop_time</span><span class="p">,</span> <span class="n">activity</span><span class="o">.</span><span class="n">start_time</span><span class="p">,</span> <span class="n">activity</span><span class="o">.</span><span class="n">stop_time</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">overlap_period</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">overlap_start_time</span><span class="p">,</span> <span class="n">overlap_stop_time</span> <span class="o">=</span> <span class="n">overlap_period</span>

                <span class="c1"># Convert the overlapping period between clip and activity to</span>
                <span class="c1"># 0-indexed start and stop frame indexes, so we can unpack 1</span>
                <span class="c1"># activity label per frame.</span>
                <span class="n">overlap_start_frame</span> <span class="o">=</span> <span class="n">seconds_to_frame_index</span><span class="p">(</span>
                    <span class="n">overlap_start_time</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frames_per_second</span>
                <span class="p">)</span>
                <span class="n">overlap_stop_frame</span> <span class="o">=</span> <span class="n">seconds_to_frame_index</span><span class="p">(</span>
                    <span class="n">overlap_stop_time</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frames_per_second</span>
                <span class="p">)</span>

                <span class="c1"># Append 1 activity label per frame</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">overlap_start_frame</span><span class="p">,</span> <span class="n">overlap_stop_frame</span><span class="p">):</span>
                    <span class="n">activities_in_clip</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activity</span><span class="p">)</span>

        <span class="c1"># Convert the list of ActivityData objects to a tensor of just the activity class IDs</span>
        <span class="n">activity_class_ids</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">activities_in_clip</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">activity_id</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">activities_in_clip</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="n">activity_class_ids_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">activity_class_ids</span><span class="p">)</span>

        <span class="n">clip_data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;video_id&quot;</span><span class="p">:</span> <span class="n">clip</span><span class="o">.</span><span class="n">video_id</span><span class="p">,</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_videos</span><span class="p">[</span><span class="n">clip</span><span class="o">.</span><span class="n">video_id</span><span class="p">]</span><span class="o">.</span><span class="n">get_clip</span><span class="p">(</span><span class="n">clip</span><span class="o">.</span><span class="n">start_time</span><span class="p">,</span> <span class="n">clip</span><span class="o">.</span><span class="n">stop_time</span><span class="p">),</span>
            <span class="s2">&quot;activities&quot;</span><span class="p">:</span> <span class="n">activity_class_ids_tensor</span><span class="p">,</span>
            <span class="s2">&quot;start_time&quot;</span><span class="p">:</span> <span class="n">clip</span><span class="o">.</span><span class="n">start_time</span><span class="p">,</span>
            <span class="s2">&quot;stop_time&quot;</span><span class="p">:</span> <span class="n">clip</span><span class="o">.</span><span class="n">stop_time</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">:</span>
            <span class="n">clip_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">clip_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">clip_data</span></div>

<div class="viewcode-block" id="DomsevDataset.__len__"><a class="viewcode-back" href="../../../api/data/domsev.html#pytorchvideo.data.domsev.DomsevDataset.__len__">[docs]</a>    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            The number of video clips in the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_clips</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_transform_clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clip</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Transforms a given video clip, according to some pre-defined transforms</span>
<span class="sd">        and an optional user transform function (self._user_transform).</span>

<span class="sd">        Args:</span>
<span class="sd">            clip (Dict[str, Any]): The clip that will be transformed.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (Dict[str, Any]) The transformed clip.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">clip</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">clip</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">clip</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_user_transform</span><span class="p">:</span>
            <span class="n">clip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_user_transform</span><span class="p">(</span><span class="n">clip</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">clip</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, PyTorchVideo contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>




  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://kalyanvasudev.github.io/ptv_temp/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://kalyanvasudev.github.io/ptv_temp/">Home</a>
          </li>
          <li>
            <a href="https://kalyanvasudev.github.io/ptv_temp/docs/tutorial_overview">Tutorials</a>
          </li>
          <li>
            <a href="https://github.com/facebookresearch/pytorchvideo/">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>